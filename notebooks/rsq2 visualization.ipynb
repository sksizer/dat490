{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Goal\n",
    "## Original Questions\n",
    "- Can we use Chronic Health Conditions to accurately predict Health Care Access?\n",
    "- Are there Demographic clusters that are disproportionately affected by Chronic Health Conditions?\n",
    "- Can unsupervised learning methods reveal distinct clusters that account for the bulk of Chronic Health Conditions?\n",
    "\n",
    "### Questions:\n",
    "- I have gotten a bit hung up bc as worded 2 and 3 seem to be asking same thing?\n",
    "- were there established chronic health conditions?\n",
    "- established demographic features?\n",
    "- what are the features used in RQ1, and RQ3?\n",
    "\n",
    "#### Rough Plan:\n",
    "Question: Refinement:\n",
    "Are there diagnostically useful demographic clusters that indicate chronic health conditions?\n",
    "- Are there demographic clusters that strongly indicate certain chronic health conditions?\n",
    "- Can we predict chronic health conditions from demographics, and how does a ML model compare with simpler cluster membership?\n",
    "\n",
    "##### Part 1: Clustering\n",
    "- Cluster the demographic features of the BRFSS data\n",
    "- Visualize clusters and prevalence of chronic health conditions within each cluster\n",
    "    - what chronic health conditions to use?\n",
    "    - VISUAL: Clustering results\n",
    "    - VISUAL:  Heatmaps of cluster membership vs chronic health conditions\n",
    "- Run statistical tests to determine if certain clusters are significantly more affected by chronic health conditions\n",
    "    - Translation - test the strength of correlation between cluster membership and chronic health conditions\n",
    "    - Does being a member of a cluster correlate with having a chronic health condition?\n",
    "#### Part 2: Prediction of Chronic Health Conditions From Demographics via DL Model. Inversion of Question 3\n",
    "- use cluster labels as features?\n",
    "- compare performance of a deep learning model vs simpler clustering membership\n",
    "\n",
    "\n",
    "\n",
    "### Misc\n",
    "- potential 'linchpin' variables given we are clustering on demographics (and ran random forest on demographics)\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T19:42:48.845305Z",
     "start_time": "2025-06-15T19:42:48.074683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from IPython import get_ipython\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kmodes.kmodes import KModes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import kmodes\n",
    "    print(\"kmodes already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing kmodes...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kmodes\"])\n",
    "    import kmodes\n",
    "    print(\"kmodes installed successfully\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def is_colab():\n",
    "    return 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Set up environment and paths\n",
    "if is_colab():\n",
    "    print(\"Running in Google Colab\")\n",
    "\n",
    "    # Clone the repository if not already cloned\n",
    "    if not os.path.exists('dat490'):\n",
    "        import subprocess\n",
    "        print(\"Cloning repository...\")\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/sksizer/dat490.git'], check=True)\n",
    "        print(\"Repository cloned successfully\")\n",
    "\n",
    "    # Add the repository to Python path for imports\n",
    "    sys.path.insert(0, '/content/dat490')\n",
    "\n",
    "    # Set paths to use data from the cloned repository\n",
    "    BFRSS_DATA_PATH = 'dat490/data/LLCP2023.parquet'\n",
    "    BFRSS_CODEBOOK_PATH = 'dat490/data/codebook_USCODE23_LLCP_021924.HTML'\n",
    "    BFRSS_DESC_PATH = 'dat490/data/LLCP2023_desc.parquet'  # Additional metadata file if needed\n",
    "else:\n",
    "    print(\"Running in local environment\")\n",
    "\n",
    "    # Add parent directory to path for dat490 module imports\n",
    "    sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "    # Use local data paths\n",
    "    BFRSS_DATA_PATH = '../data/LLCP2023.parquet'\n",
    "    BFRSS_CODEBOOK_PATH = '../data/codebook_USCODE23_LLCP_021924.HTML'\n",
    "    BFRSS_DESC_PATH = '../data/LLCP2023_desc.parquet'  # Additional metadata file if needed\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"\\\\nData path: {BFRSS_DATA_PATH}\")\n",
    "print(f\"Codebook path: {BFRSS_CODEBOOK_PATH}\")\n",
    "\n",
    "if not os.path.exists(BFRSS_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Data file not found at {BFRSS_DATA_PATH}\")\n",
    "\n",
    "if not os.path.exists(BFRSS_CODEBOOK_PATH):\n",
    "    raise FileNotFoundError(f\"Codebook file not found at {BFRSS_CODEBOOK_PATH}\")\n",
    "\n",
    "print(\"\\\\nAll required files found!\")\n",
    "logger.info('Environment setup complete')\n",
    "\n",
    "##################################\n",
    "# Load BFRSS data and metadata using the new wrapper\n",
    "from dat490 import load_bfrss\n",
    "\n",
    "# Single function call to load everything\n",
    "# exclude_desc_columns=True will exclude _DESC columns from metadata generation\n",
    "bfrss = load_bfrss(exclude_desc_columns=True)\n",
    "\n",
    "# Get a copy of the raw DataFrame\n",
    "bfrss_raw_df = bfrss.cloneDF()\n",
    "bfrss_raw_df.info()\n",
    "\n",
    "DEMOGRAPHIC_FEATURE_COLUMNS = [\n",
    "    # Demographics section columns (13 total)\n",
    "    # Demographics section columns (13 total)\n",
    "    'MARITAL',    # https://singular-eclair-6a5a16.netlify.app/columns/MARITAL\n",
    "    'EDUCA',      # https://singular-eclair-6a5a16.netlify.app/columns/EDUCA\n",
    "    'RENTHOM1',   # https://singular-eclair-6a5a16.netlify.app/columns/RENTHOM1\n",
    "    # 'NUMHHOL4',   # https://singular-eclair-6a5a16.netlify.app/columns/NUMHHOL4\n",
    "    # 'NUMPHON4',   # https://singular-eclair-6a5a16.netlify.app/columns/NUMPHON4\n",
    "    # 'CPDEMO1C',   # https://singular-eclair-6a5a16.netlify.app/columns/CPDEMO1C\n",
    "    'VETERAN3',   # https://singular-eclair-6a5a16.netlify.app/columns/VETERAN3\n",
    "    'EMPLOY1',    # https://singular-eclair-6a5a16.netlify.app/columns/EMPLOY1\n",
    "    # 'CHILDREN',   # https://singular-eclair-6a5a16.netlify.app/columns/CHILDREN\n",
    "    'INCOME3',    # https://singular-eclair-6a5a16.netlify.app/columns/INCOME3\n",
    "    # 'PREGNANT',   # https://singular-eclair-6a5a16.netlify.app/columns/PREGNANT\n",
    "    'SEXVAR',    # https://singular-eclair-6a5a16.netlify.app/columns/SEXVAR\n",
    "    '_HISPANC', # https://singular-eclair-6a5a16.netlify.app/columns/_HISPANC # Calculated but not sure from what\n",
    "    # '_CRACE1',    # https://singular-eclair-6a5a16.netlify.app/columns/_CRACE1 # Child race\n",
    "    '_IMPRACE',   # https://singular-eclair-6a5a16.netlify.app/columns/_IMPRACE\n",
    "    # '_AGE80',     # https://singular-eclair-6a5a16.netlify.app/columns/_AGE80\n",
    "]\n",
    "\n",
    "\n"
   ],
   "id": "3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kmodes already installed\n",
      "Running in local environment\n",
      "\\nData path: ../data/LLCP2023.parquet\n",
      "Codebook path: ../data/codebook_USCODE23_LLCP_021924.HTML\n",
      "\\nAll required files found!\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 433323 entries, 0 to 433322\n",
      "Columns: 694 entries, _STATE to X_DRNKDRV_DESC\n",
      "dtypes: category(342), float64(340), int32(5), object(7)\n",
      "memory usage: 1.3+ GB\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# K-Modes Analysis of BRFSS Data\n",
    "\n",
    "K-Modes clustering is an extension of K-Means designed for categorical data. Instead of using means to define cluster centers, K-Modes uses modes (most frequent values) and measures dissimilarity using the number of mismatches between data points."
   ]
  },
  {
   "cell_type": "code",
   "source": "# Subsampling and Stability Testing for K-Modes\n\nimport numpy as np\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef subsample_kmode_stability(df: pd.DataFrame, \n                            feature_cols: list, \n                            n_clusters: int = 2,\n                            n_iterations: int = 10,\n                            subsample_ratio: float = 0.8,\n                            random_state: int = 42):\n    \"\"\"\n    Perform stability testing of k-modes clustering through subsampling.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Input dataframe\n    feature_cols : list\n        List of feature columns to use for clustering\n    n_clusters : int\n        Number of clusters for k-modes\n    n_iterations : int\n        Number of subsampling iterations\n    subsample_ratio : float\n        Proportion of data to sample in each iteration\n    random_state : int\n        Random seed for reproducibility\n        \n    Returns:\n    --------\n    dict : Dictionary containing stability metrics and results\n    \"\"\"\n    \n    np.random.seed(random_state)\n    \n    # Prepare data\n    data = df[feature_cols].copy()\n    \n    # Encode categorical variables\n    encoders = {}\n    for col in data.columns:\n        if data[col].dtype == 'object' or data[col].dtype.name == 'category':\n            le = LabelEncoder()\n            data[col] = data[col].fillna(data[col].mode()[0] if len(data[col].mode()) > 0 else 'missing')\n            data[col] = le.fit_transform(data[col])\n            encoders[col] = le\n        else:\n            data[col] = data[col].fillna(data[col].mean())\n    \n    # Storage for results\n    results = {\n        'cluster_assignments': [],\n        'centroids': [],\n        'costs': [],\n        'ari_scores': [],\n        'nmi_scores': [],\n        'stability_scores': []\n    }\n    \n    # Reference clustering on full dataset\n    km_ref = KModes(n_clusters=n_clusters, init='Huang', n_init=5, verbose=0)\n    ref_clusters = km_ref.fit_predict(data)\n    ref_centroids = km_ref.cluster_centroids_\n    \n    print(f\"Running {n_iterations} iterations with {subsample_ratio*100:.0f}% subsampling...\")\n    \n    for i in range(n_iterations):\n        # Subsample data\n        n_samples = int(len(data) * subsample_ratio)\n        sample_idx = np.random.choice(len(data), n_samples, replace=False)\n        data_sample = data.iloc[sample_idx]\n        \n        # Run k-modes on subsample\n        km = KModes(n_clusters=n_clusters, init='Huang', n_init=5, verbose=0)\n        clusters = km.fit_predict(data_sample)\n        \n        # Store results\n        results['cluster_assignments'].append(clusters)\n        results['centroids'].append(km.cluster_centroids_)\n        results['costs'].append(km.cost_)\n        \n        # Calculate stability metrics against reference (for overlapping samples)\n        ref_clusters_sample = ref_clusters[sample_idx]\n        ari = adjusted_rand_score(ref_clusters_sample, clusters)\n        nmi = normalized_mutual_info_score(ref_clusters_sample, clusters)\n        \n        results['ari_scores'].append(ari)\n        results['nmi_scores'].append(nmi)\n        \n        if (i + 1) % 5 == 0:\n            print(f\"  Completed iteration {i+1}/{n_iterations}\")\n    \n    # Calculate pairwise stability between iterations\n    print(\"\\\\nCalculating pairwise stability...\")\n    for i in range(n_iterations):\n        for j in range(i+1, n_iterations):\n            # Find common indices between two subsamples\n            # Since we're using random sampling, we need to track indices\n            # For simplicity, we'll calculate stability on the full dataset predictions\n            pass\n    \n    # Summary statistics\n    results['summary'] = {\n        'mean_cost': np.mean(results['costs']),\n        'std_cost': np.std(results['costs']),\n        'mean_ari': np.mean(results['ari_scores']),\n        'std_ari': np.std(results['ari_scores']),\n        'mean_nmi': np.mean(results['nmi_scores']),\n        'std_nmi': np.std(results['nmi_scores']),\n        'reference_centroids': ref_centroids,\n        'reference_clusters': ref_clusters\n    }\n    \n    return results",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T19:42:48.862206Z",
     "start_time": "2025-06-15T19:42:48.856477Z"
    }
   },
   "id": "4edd583423591466",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "# Advanced Stability Metrics with Pairwise Comparisons\n\ndef calculate_pairwise_stability(df: pd.DataFrame,\n                               feature_cols: list,\n                               n_clusters: int = 2,\n                               n_iterations: int = 10,\n                               subsample_ratio: float = 0.8,\n                               random_state: int = 42):\n    \"\"\"\n    Calculate pairwise stability between multiple k-modes runs with bootstrap confidence intervals.\n    \"\"\"\n    \n    np.random.seed(random_state)\n    \n    # Prepare data\n    data = df[feature_cols].copy()\n    \n    # Encode categorical variables\n    for col in data.columns:\n        if data[col].dtype == 'object' or data[col].dtype.name == 'category':\n            le = LabelEncoder()\n            data[col] = data[col].fillna(data[col].mode()[0] if len(data[col].mode()) > 0 else 'missing')\n            data[col] = le.fit_transform(data[col])\n        else:\n            data[col] = data[col].fillna(data[col].mean())\n    \n    # Store sample indices and cluster assignments\n    sample_indices = []\n    cluster_assignments = []\n    centroids = []\n    \n    print(f\"Running {n_iterations} k-modes iterations...\")\n    \n    for i in range(n_iterations):\n        # Subsample data\n        n_samples = int(len(data) * subsample_ratio)\n        sample_idx = np.random.choice(len(data), n_samples, replace=False)\n        # Sort indices to use searchsorted later\n        sample_idx = np.sort(sample_idx)\n        sample_indices.append(sample_idx)\n        \n        data_sample = data.iloc[sample_idx]\n        \n        # Run k-modes\n        km = KModes(n_clusters=n_clusters, init='Huang', n_init=5, verbose=0)\n        clusters = km.fit_predict(data_sample)\n        \n        cluster_assignments.append(clusters)\n        centroids.append(km.cluster_centroids_)\n        \n        if (i + 1) % 5 == 0:\n            print(f\"  Completed iteration {i+1}/{n_iterations}\")\n    \n    # Calculate pairwise stability\n    print(\"\\\\nCalculating pairwise stability metrics...\")\n    pairwise_ari = np.zeros((n_iterations, n_iterations))\n    pairwise_nmi = np.zeros((n_iterations, n_iterations))\n    \n    for i in range(n_iterations):\n        for j in range(i, n_iterations):\n            if i == j:\n                pairwise_ari[i, j] = 1.0\n                pairwise_nmi[i, j] = 1.0\n            else:\n                # Find common indices\n                common_idx = np.intersect1d(sample_indices[i], sample_indices[j])\n                \n                if len(common_idx) > 0:\n                    # Get positions in respective samples\n                    pos_i = np.searchsorted(sample_indices[i], common_idx)\n                    pos_j = np.searchsorted(sample_indices[j], common_idx)\n                    \n                    # Compare cluster assignments for common samples\n                    clusters_i = cluster_assignments[i][pos_i]\n                    clusters_j = cluster_assignments[j][pos_j]\n                    \n                    ari = adjusted_rand_score(clusters_i, clusters_j)\n                    nmi = normalized_mutual_info_score(clusters_i, clusters_j)\n                    \n                    pairwise_ari[i, j] = pairwise_ari[j, i] = ari\n                    pairwise_nmi[i, j] = pairwise_nmi[j, i] = nmi\n    \n    # Calculate centroid stability\n    centroid_distances = np.zeros((n_iterations, n_iterations))\n    for i in range(n_iterations):\n        for j in range(i, n_iterations):\n            if i == j:\n                centroid_distances[i, j] = 0.0\n            else:\n                # Calculate Hamming distance between centroids\n                dist = 0\n                for k in range(n_clusters):\n                    dist += np.sum(centroids[i][k] != centroids[j][k])\n                centroid_distances[i, j] = centroid_distances[j, i] = dist / (n_clusters * len(feature_cols))\n    \n    return {\n        'pairwise_ari': pairwise_ari,\n        'pairwise_nmi': pairwise_nmi,\n        'centroid_distances': centroid_distances,\n        'mean_ari': np.mean(pairwise_ari[np.triu_indices(n_iterations, k=1)]),\n        'std_ari': np.std(pairwise_ari[np.triu_indices(n_iterations, k=1)]),\n        'mean_nmi': np.mean(pairwise_nmi[np.triu_indices(n_iterations, k=1)]),\n        'std_nmi': np.std(pairwise_nmi[np.triu_indices(n_iterations, k=1)]),\n        'mean_centroid_dist': np.mean(centroid_distances[np.triu_indices(n_iterations, k=1)]),\n        'std_centroid_dist': np.std(centroid_distances[np.triu_indices(n_iterations, k=1)])\n    }",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T19:42:48.879040Z",
     "start_time": "2025-06-15T19:42:48.873313Z"
    }
   },
   "id": "c9752d9c3011ffe4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualization Functions for Stability Results\n\ndef visualize_stability_results(results, title_prefix=\"\"):\n    \"\"\"\n    Create comprehensive visualizations for k-modes stability analysis.\n    \"\"\"\n    \n    # Set up the figure with subplots\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    fig.suptitle(f'{title_prefix}K-Modes Clustering Stability Analysis', fontsize=16)\n    \n    # 1. Stability Metrics Over Iterations (if from subsample_kmode_stability)\n    if 'ari_scores' in results:\n        ax = axes[0, 0]\n        iterations = range(1, len(results['ari_scores']) + 1)\n        \n        ax.plot(iterations, results['ari_scores'], 'o-', label='ARI', markersize=8)\n        ax.plot(iterations, results['nmi_scores'], 's-', label='NMI', markersize=8)\n        \n        # Add mean lines\n        ax.axhline(y=results['summary']['mean_ari'], color='blue', linestyle='--', alpha=0.5)\n        ax.axhline(y=results['summary']['mean_nmi'], color='orange', linestyle='--', alpha=0.5)\n        \n        ax.set_xlabel('Iteration')\n        ax.set_ylabel('Score')\n        ax.set_title('Stability Metrics vs Reference Clustering')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax.set_ylim(0, 1.05)\n    \n    # 2. Pairwise Stability Heatmap (if from calculate_pairwise_stability)\n    if 'pairwise_ari' in results:\n        ax = axes[0, 1]\n        sns.heatmap(results['pairwise_ari'], annot=True, fmt='.2f', cmap='YlOrRd', \n                   vmin=0, vmax=1, square=True, ax=ax, cbar_kws={'label': 'ARI'})\n        ax.set_title('Pairwise ARI Between Iterations')\n        ax.set_xlabel('Iteration')\n        ax.set_ylabel('Iteration')\n    \n    # 3. Distribution of Stability Scores\n    if 'pairwise_ari' in results:\n        ax = axes[1, 0]\n        \n        # Extract upper triangle values (excluding diagonal)\n        n_iter = results['pairwise_ari'].shape[0]\n        ari_values = results['pairwise_ari'][np.triu_indices(n_iter, k=1)]\n        nmi_values = results['pairwise_nmi'][np.triu_indices(n_iter, k=1)]\n        \n        # Create violin plots\n        data_to_plot = [ari_values, nmi_values]\n        positions = [1, 2]\n        \n        parts = ax.violinplot(data_to_plot, positions=positions, showmeans=True, showmedians=True)\n        \n        # Customize colors\n        colors = ['#3498db', '#e74c3c']\n        for pc, color in zip(parts['bodies'], colors):\n            pc.set_facecolor(color)\n            pc.set_alpha(0.7)\n        \n        ax.set_xticks(positions)\n        ax.set_xticklabels(['ARI', 'NMI'])\n        ax.set_ylabel('Score')\n        ax.set_title('Distribution of Pairwise Stability Scores')\n        ax.set_ylim(0, 1.05)\n        ax.grid(True, axis='y', alpha=0.3)\n        \n        # Add summary statistics\n        ax.text(1, 0.05, f'μ={np.mean(ari_values):.3f}\\\\nσ={np.std(ari_values):.3f}', \n                ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n        ax.text(2, 0.05, f'μ={np.mean(nmi_values):.3f}\\\\nσ={np.std(nmi_values):.3f}', \n                ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    \n    # 4. Centroid Stability\n    if 'centroid_distances' in results:\n        ax = axes[1, 1]\n        \n        # Create a histogram of centroid distances\n        distances = results['centroid_distances'][np.triu_indices(results['centroid_distances'].shape[0], k=1)]\n        \n        ax.hist(distances, bins=20, edgecolor='black', alpha=0.7)\n        ax.set_xlabel('Normalized Hamming Distance')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Distribution of Centroid Distances')\n        ax.grid(True, axis='y', alpha=0.3)\n        \n        # Add mean line\n        mean_dist = np.mean(distances)\n        ax.axvline(x=mean_dist, color='red', linestyle='--', linewidth=2, \n                  label=f'Mean = {mean_dist:.3f}')\n        ax.legend()\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    # Print summary statistics\n    print(\"\\\\n=== Stability Analysis Summary ===\")\n    if 'summary' in results:\n        print(f\"Mean ARI: {results['summary']['mean_ari']:.3f} ± {results['summary']['std_ari']:.3f}\")\n        print(f\"Mean NMI: {results['summary']['mean_nmi']:.3f} ± {results['summary']['std_nmi']:.3f}\")\n    elif 'mean_ari' in results:\n        print(f\"Mean Pairwise ARI: {results['mean_ari']:.3f} ± {results['std_ari']:.3f}\")\n        print(f\"Mean Pairwise NMI: {results['mean_nmi']:.3f} ± {results['std_nmi']:.3f}\")\n        print(f\"Mean Centroid Distance: {results['mean_centroid_dist']:.3f} ± {results['std_centroid_dist']:.3f}\")\n    \n    return fig",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T19:42:48.898149Z",
     "start_time": "2025-06-15T19:42:48.891096Z"
    }
   },
   "id": "9c8c850799d593b0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "# Helper Functions for Quick Testing and Multi-Scale Analysis\n\ndef quick_stability_test(df: pd.DataFrame,\n                        feature_cols: list,\n                        n_clusters: int = 2,\n                        n_iterations: int = 5,\n                        subsample_size: int = 1000,  # Absolute sample size instead of ratio\n                        random_state: int = 42):\n    \"\"\"\n    Quick stability test with small absolute sample sizes for proof of concept.\n    \n    Parameters:\n    -----------\n    subsample_size : int\n        Absolute number of samples to use (default 1000 for quick testing)\n    \"\"\"\n    \n    # Calculate the ratio based on absolute size\n    total_rows = len(df)\n    subsample_ratio = min(subsample_size / total_rows, 1.0)\n    \n    print(f\"\\\\n=== Quick Stability Test ===\")\n    print(f\"Total rows: {total_rows:,}\")\n    print(f\"Sample size: {subsample_size:,} ({subsample_ratio*100:.1f}% of data)\")\n    print(f\"Iterations: {n_iterations}\")\n    print(f\"Clusters: {n_clusters}\\\\n\")\n    \n    # Run the pairwise stability analysis with small samples\n    results = calculate_pairwise_stability(\n        df,\n        feature_cols,\n        n_clusters=n_clusters,\n        n_iterations=n_iterations,\n        subsample_ratio=subsample_ratio,\n        random_state=random_state\n    )\n    \n    return results\n\n\ndef multi_scale_stability_analysis(df: pd.DataFrame,\n                                 feature_cols: list,\n                                 n_clusters: int = 2,\n                                 sample_sizes: list = [500, 1000, 5000, 10000],\n                                 n_iterations: int = 10,\n                                 random_state: int = 42):\n    \"\"\"\n    Run stability analysis across multiple sample sizes to understand scaling behavior.\n    \"\"\"\n    \n    results_by_size = {}\n    \n    for sample_size in sample_sizes:\n        print(f\"\\\\n{'='*50}\")\n        print(f\"Testing with sample size: {sample_size:,}\")\n        \n        results = quick_stability_test(\n            df,\n            feature_cols,\n            n_clusters=n_clusters,\n            n_iterations=n_iterations,\n            subsample_size=sample_size,\n            random_state=random_state\n        )\n        \n        results_by_size[sample_size] = results\n    \n    # Create summary plot\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Plot 1: Stability metrics vs sample size\n    sizes = list(results_by_size.keys())\n    mean_aris = [results_by_size[s]['mean_ari'] for s in sizes]\n    std_aris = [results_by_size[s]['std_ari'] for s in sizes]\n    mean_nmis = [results_by_size[s]['mean_nmi'] for s in sizes]\n    std_nmis = [results_by_size[s]['std_nmi'] for s in sizes]\n    \n    ax1.errorbar(sizes, mean_aris, yerr=std_aris, marker='o', label='ARI', capsize=5, markersize=8)\n    ax1.errorbar(sizes, mean_nmis, yerr=std_nmis, marker='s', label='NMI', capsize=5, markersize=8)\n    \n    ax1.set_xlabel('Sample Size')\n    ax1.set_ylabel('Mean Stability Score')\n    ax1.set_title('Stability vs Sample Size')\n    ax1.set_xscale('log')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    ax1.set_ylim(0, 1.05)\n    \n    # Plot 2: Centroid distance vs sample size\n    mean_dists = [results_by_size[s]['mean_centroid_dist'] for s in sizes]\n    std_dists = [results_by_size[s]['std_centroid_dist'] for s in sizes]\n    \n    ax2.errorbar(sizes, mean_dists, yerr=std_dists, marker='d', color='green', capsize=5, markersize=8)\n    ax2.set_xlabel('Sample Size')\n    ax2.set_ylabel('Mean Centroid Distance')\n    ax2.set_title('Centroid Stability vs Sample Size')\n    ax2.set_xscale('log')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.suptitle('K-Modes Stability Analysis Across Sample Sizes', fontsize=16)\n    plt.tight_layout()\n    \n    return results_by_size",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T19:42:48.908768Z",
     "start_time": "2025-06-15T19:42:48.903411Z"
    }
   },
   "id": "a60da4c95be6847b",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "## Quick Test Run (Small Sample)\n\n# This cell runs a quick stability test with small sample sizes\n# Run this for rapid prototyping and testing\n\nprint(\"=\"*60)\nprint(\"QUICK TEST RUN - Small Sample Sizes\")\nprint(\"=\"*60)\n\n# Single quick test\nquick_results = quick_stability_test(\n    bfrss_raw_df,\n    DEMOGRAPHIC_FEATURE_COLUMNS,\n    n_clusters=2,\n    n_iterations=5,\n    subsample_size=1000  # Only 1000 samples\n)\n\n# Visualize results\nvisualize_stability_results(quick_results, \"Quick Test - \")\n\n# Multi-scale analysis with small samples\nprint(\"\\\\n\" + \"=\"*60)\nprint(\"Multi-Scale Analysis with Small Samples\")\nprint(\"=\"*60)\n\nmulti_results_quick = multi_scale_stability_analysis(\n    bfrss_raw_df,\n    DEMOGRAPHIC_FEATURE_COLUMNS,\n    n_clusters=2,\n    sample_sizes=[500, 1000, 2000],  # Very small sizes\n    n_iterations=5  # Fewer iterations\n)",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T19:42:51.310289Z",
     "start_time": "2025-06-15T19:42:50.558412Z"
    }
   },
   "id": "d29d3f3302ae3a7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUICK TEST RUN - Small Sample Sizes\n",
      "============================================================\n",
      "\\n=== Quick Stability Test ===\n",
      "Total rows: 433,323\n",
      "Sample size: 1,000 (0.2% of data)\n",
      "Iterations: 5\n",
      "Clusters: 2\\n\n",
      "Running 5 k-modes iterations...\n",
      "  Completed iteration 5/5\n",
      "\\nCalculating pairwise stability metrics...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 999 is out of bounds for axis 0 with size 999",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Single quick test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m quick_results = \u001b[43mquick_stability_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbfrss_raw_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mDEMOGRAPHIC_FEATURE_COLUMNS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Only 1000 samples\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[32m     20\u001b[39m visualize_stability_results(quick_results, \u001b[33m\"\u001b[39m\u001b[33mQuick Test - \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mquick_stability_test\u001b[39m\u001b[34m(df, feature_cols, n_clusters, n_iterations, subsample_size, random_state)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mClusters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_clusters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Run the pairwise stability analysis with small samples\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m results = \u001b[43mcalculate_pairwise_stability\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubsample_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsample_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mcalculate_pairwise_stability\u001b[39m\u001b[34m(df, feature_cols, n_clusters, n_iterations, subsample_ratio, random_state)\u001b[39m\n\u001b[32m     69\u001b[39m pos_j = np.searchsorted(sample_indices[j], common_idx)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Compare cluster assignments for common samples\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m clusters_i = \u001b[43mcluster_assignments\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpos_i\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     73\u001b[39m clusters_j = cluster_assignments[j][pos_j]\n\u001b[32m     75\u001b[39m ari = adjusted_rand_score(clusters_i, clusters_j)\n",
      "\u001b[31mIndexError\u001b[39m: index 999 is out of bounds for axis 0 with size 999"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "## Full Analysis Run (Large Sample)\n\n# This cell runs a comprehensive stability analysis with larger sample sizes\n# WARNING: This will take significantly longer to run\n\nprint(\"=\"*60)\nprint(\"FULL ANALYSIS RUN - Large Sample Sizes\")\nprint(\"=\"*60)\nprint(\"WARNING: This analysis will take several minutes to complete\")\nprint(\"=\"*60)\n\n# Comprehensive stability test using subsample approach\nfull_subsample_results = subsample_kmode_stability(\n    bfrss_raw_df,\n    DEMOGRAPHIC_FEATURE_COLUMNS,\n    n_clusters=2,\n    n_iterations=20,  # More iterations\n    subsample_ratio=0.8  # 80% of data\n)\n\n# Visualize subsample results\nvisualize_stability_results(full_subsample_results, \"Full Subsample Analysis - \")\n\n# Comprehensive pairwise stability analysis\nprint(\"\\\\n\" + \"=\"*60)\nprint(\"Pairwise Stability Analysis\")\nprint(\"=\"*60)\n\nfull_pairwise_results = calculate_pairwise_stability(\n    bfrss_raw_df,\n    DEMOGRAPHIC_FEATURE_COLUMNS,\n    n_clusters=2,\n    n_iterations=15,\n    subsample_ratio=0.8\n)\n\n# Visualize pairwise results\nvisualize_stability_results(full_pairwise_results, \"Full Pairwise Analysis - \")\n\n# Multi-scale analysis with larger samples\nprint(\"\\\\n\" + \"=\"*60)\nprint(\"Multi-Scale Analysis with Large Samples\")\nprint(\"=\"*60)\n\nmulti_results_full = multi_scale_stability_analysis(\n    bfrss_raw_df,\n    DEMOGRAPHIC_FEATURE_COLUMNS,\n    n_clusters=2,\n    sample_sizes=[5000, 10000, 20000, 50000],  # Larger sizes\n    n_iterations=10  # More iterations\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "id": "efc48128a25cdcd4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}