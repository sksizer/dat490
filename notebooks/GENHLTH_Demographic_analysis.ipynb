{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENHLTH Demographic Analysis\n",
    "## Predicting General Health Status from Demographics using Random Forest\n",
    "\n",
    "This notebook analyzes the relationship between demographic variables and general health status (GENHLTH) in the BRFSS dataset using a Random Forest machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def is_colab():\n",
    "    return 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Setup for Google Colab environment only\n",
    "if is_colab():\n",
    "    GIT_REPO_URL = 'https://github.com/sksizer/dat490.git'\n",
    "    LOCAL_DIR = '/content/code/dat490'\n",
    "\n",
    "    if not os.path.exists(LOCAL_DIR):\n",
    "        print(f\"Cloning repo into {LOCAL_DIR}...\")\n",
    "        subprocess.run(['git', 'clone', GIT_REPO_URL, LOCAL_DIR], check=True)\n",
    "    else:\n",
    "        print(f\"Repo already exists at {LOCAL_DIR}, pulling latest changes...\")\n",
    "        subprocess.run(['git', '-C', LOCAL_DIR, 'pull'], check=True)\n",
    "\n",
    "    if LOCAL_DIR not in sys.path:\n",
    "        sys.path.insert(0, LOCAL_DIR)\n",
    "        print(f\"Added {LOCAL_DIR} to sys.path\")\n",
    "\n",
    "    # Import dat490 package\n",
    "    import dat490\n",
    "else:\n",
    "    # Running locally - assume dat490 is already available\n",
    "    import dat490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BFRSS data and metadata using the new wrapper with destructured assignment\n",
    "from dat490 import load_bfrss_components\n",
    "\n",
    "# Set root directory based on environment\n",
    "if is_colab():\n",
    "    root_directory = '/content/code/dat490'\n",
    "else:\n",
    "    root_directory = None  # Use default search paths\n",
    "\n",
    "# Single function call to load everything with destructured assignment and semantic null conversion\n",
    "df, metadata, logger = load_bfrss_components(semantically_null=True, root_dir=root_directory)\n",
    "\n",
    "logger.info(f\"DataFrame shape: {df.shape}\")\n",
    "logger.info(f\"Metadata entries: {len(metadata)}\")\n",
    "\n",
    "# Examine GENHLTH variable\n",
    "if 'GENHLTH' in df.columns:\n",
    "    genhlth_meta = metadata.get('GENHLTH')\n",
    "    if genhlth_meta:\n",
    "        print(f\"Column: {genhlth_meta.sas_variable_name}\")\n",
    "        print(f\"Label: {genhlth_meta.label}\")\n",
    "        print(f\"Question: {genhlth_meta.question}\")\n",
    "        print(f\"\\nValue mappings:\")\n",
    "        for value, description in genhlth_meta.value_lookup.items():\n",
    "            print(f\"  {value}: {description}\")\n",
    "    \n",
    "    # Show value counts\n",
    "    print(\"\\nGENHLTH Distribution (after semantic null conversion):\")\n",
    "    genhlth_counts = df['GENHLTH'].value_counts().sort_index()\n",
    "    for value, count in genhlth_counts.items():\n",
    "        if not pd.isna(value) and genhlth_meta:\n",
    "            description = genhlth_meta.value_lookup.get(int(value), f\"Code {value}\")\n",
    "            print(f\"  {int(value)}: {description} (Count: {count:,})\")\n",
    "        else:\n",
    "            print(f\"  {value}: Missing (Count: {count:,})\")\n",
    "    \n",
    "    # Check NaN count to confirm semantic nulls were converted\n",
    "    nan_count = df['GENHLTH'].isna().sum()\n",
    "    print(f\"\\nTotal NaN values (including converted semantic nulls): {nan_count:,}\")\n",
    "else:\n",
    "    print(\"GENHLTH column not found. Searching for similar health status variables...\")\n",
    "    health_cols = [col for col in df.columns if 'HLTH' in col or 'HEALTH' in col]\n",
    "    print(f\"Health-related columns found: {health_cols}\")\n",
    "\n",
    "# Store GENHLTH metadata for later use in confusion matrix\n",
    "genhlth_metadata = metadata.get('GENHLTH') if 'GENHLTH' in df.columns else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Target Variable: GENHLTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Demographics Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Demographics section features\n",
    "demographics_columns = [\n",
    "    col for col, meta in metadata.items()\n",
    "    if hasattr(meta, 'section_name') and 'Demographics' in meta.section_name\n",
    "]\n",
    "\n",
    "print(f\"Demographics columns ({len(demographics_columns)}): {demographics_columns}\")\n",
    "\n",
    "# Also look for calculated demographic variables\n",
    "calc_demo_columns = [\n",
    "    col for col in df.columns \n",
    "    if col.startswith('_') and any(demo in col.upper() for demo in ['AGE', 'RACE', 'SEX', 'INCOME', 'EDUC'])\n",
    "]\n",
    "\n",
    "print(f\"\\nCalculated demographic columns: {calc_demo_columns}\")\n",
    "\n",
    "# Combine and filter to available columns\n",
    "all_demo_features = list(set(demographics_columns + calc_demo_columns))\n",
    "available_demo_features = [col for col in all_demo_features if col in df.columns]\n",
    "\n",
    "print(f\"\\nFinal demographics features to use ({len(available_demo_features)}): {available_demo_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for key demographic variables\n",
    "key_demo_vars = ['MARITAL', 'EDUCA', 'EMPLOY1', 'INCOME3']\n",
    "available_key_vars = [var for var in key_demo_vars if var in df.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, var in enumerate(available_key_vars[:4]):\n",
    "    if var in metadata:\n",
    "        var_meta = metadata[var]\n",
    "        value_counts = df[var].value_counts().head(8)\n",
    "        \n",
    "        # Map values to descriptions\n",
    "        labels = []\n",
    "        for val in value_counts.index:\n",
    "            if not pd.isna(val):\n",
    "                desc = var_meta.value_lookup.get(int(val), f\"Code {val}\")\n",
    "                # Truncate long labels\n",
    "                if len(desc) > 30:\n",
    "                    desc = desc[:27] + \"...\"\n",
    "                labels.append(desc)\n",
    "            else:\n",
    "                labels.append(\"Missing\")\n",
    "        \n",
    "        axes[i].barh(range(len(labels)), value_counts.values)\n",
    "        axes[i].set_yticks(range(len(labels)))\n",
    "        axes[i].set_yticklabels(labels, fontsize=8)\n",
    "        axes[i].set_title(f\"{var}: {var_meta.label}\", fontsize=10)\n",
    "        axes[i].set_xlabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Values\n",
    "\n",
    "With `semantically_null=True`, values that indicate missing/refused/unknown data have already been converted to NaN during data loading. This eliminates the need for manual missing value detection and replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify semantic null conversion worked correctly\n",
    "target_col = 'GENHLTH'\n",
    "\n",
    "print(f\"Semantic Null Conversion Summary:\")\n",
    "print(f\"Target variable: {target_col}\")\n",
    "\n",
    "if target_col in df.columns:\n",
    "    # Check current value distribution\n",
    "    value_counts = df[target_col].value_counts().sort_index()\n",
    "    total_count = len(df)\n",
    "    nan_count = df[target_col].isna().sum()\n",
    "    \n",
    "    print(f\"\\nCurrent distribution:\")\n",
    "    for value, count in value_counts.items():\n",
    "        if not pd.isna(value):\n",
    "            pct = count / total_count * 100\n",
    "            print(f\"  {int(value)}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(f\"  NaN: {nan_count:,} ({nan_count/total_count*100:.1f}%)\")\n",
    "    \n",
    "    # Verify no missing indicators remain (should be no 7s or 9s for GENHLTH)\n",
    "    missing_indicators_found = df[target_col].isin([7, 9]).sum()\n",
    "    if missing_indicators_found == 0:\n",
    "        print(f\"\\n✓ Success: No missing indicator values (7, 9) found in target variable\")\n",
    "        print(f\"  Semantic nulls have been properly converted to NaN\")\n",
    "    else:\n",
    "        print(f\"\\n⚠ Warning: Found {missing_indicators_found} missing indicators still present\")\n",
    "\n",
    "print(f\"\\nData is now ready for analysis with proper missing value handling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "target_col = 'GENHLTH'\n",
    "\n",
    "if target_col not in df.columns:\n",
    "    print(f\"Target variable {target_col} not found!\")\n",
    "    # Look for alternative health status variables\n",
    "    health_alternatives = [col for col in df.columns if 'HLTH' in col]\n",
    "    print(f\"Available health columns: {health_alternatives}\")\n",
    "    if health_alternatives:\n",
    "        target_col = health_alternatives[0]\n",
    "        print(f\"Using {target_col} as target variable instead\")\n",
    "    else:\n",
    "        raise ValueError(\"No suitable target variable found\")\n",
    "\n",
    "# Select demographics features that are available in the dataset\n",
    "feature_cols = [col for col in available_demo_features if col in df.columns and col != target_col]\n",
    "\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"Feature variables ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "# Create analysis dataset\n",
    "analysis_cols = [target_col] + feature_cols\n",
    "analysis_df = df[analysis_cols].copy()\n",
    "\n",
    "print(f\"\\nAnalysis dataset shape: {analysis_df.shape}\")\n",
    "print(f\"Missing values per column (all semantic nulls already converted):\")\n",
    "missing_counts = analysis_df.isnull().sum()\n",
    "for col in analysis_cols:\n",
    "    missing_pct = missing_counts[col] / len(analysis_df) * 100\n",
    "    print(f\"  {col}: {missing_counts[col]:,} ({missing_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset for modeling (semantic nulls already converted)\n",
    "target_col = 'GENHLTH'\n",
    "\n",
    "# Remove rows with missing target values (now all proper NaN values)\n",
    "model_df = analysis_df.dropna(subset=[target_col]).copy()\n",
    "print(f\"After dropping rows with missing target values: {model_df.shape}\")\n",
    "\n",
    "# Handle missing values in features using the same approach as before\n",
    "missing_threshold = 0.3  # Drop columns with >30% missing values\n",
    "\n",
    "cols_to_keep = []\n",
    "for col in feature_cols:\n",
    "    missing_pct = model_df[col].isnull().sum() / len(model_df)\n",
    "    \n",
    "    if missing_pct <= missing_threshold:\n",
    "        cols_to_keep.append(col)\n",
    "    else:\n",
    "        print(f\"Dropping {col} due to {missing_pct:.1%} missing values\")\n",
    "\n",
    "feature_cols = cols_to_keep\n",
    "print(f\"\\nFeatures after missing value filtering: {feature_cols}\")\n",
    "\n",
    "# Fill remaining missing values with mode (most common value)\n",
    "for col in feature_cols:\n",
    "    if model_df[col].isnull().any():\n",
    "        mode_value = model_df[col].mode().iloc[0] if not model_df[col].mode().empty else 0\n",
    "        filled_count = model_df[col].isnull().sum()\n",
    "        model_df[col] = model_df[col].fillna(mode_value)\n",
    "        print(f\"Filled {filled_count:,} missing values in {col} with mode: {mode_value}\")\n",
    "\n",
    "print(f\"\\nFinal model dataset shape: {model_df.shape}\")\n",
    "print(f\"Missing values remaining: {model_df.isnull().sum().sum()}\")\n",
    "\n",
    "# Show final target distribution (should now be clean with no missing indicators)\n",
    "print(f\"\\nFinal target variable distribution:\")\n",
    "target_counts = model_df[target_col].value_counts().sort_index()\n",
    "target_meta = metadata.get(target_col)\n",
    "for value, count in target_counts.items():\n",
    "    if not pd.isna(value) and target_meta:\n",
    "        description = target_meta.value_lookup.get(int(value), f\"Code {value}\")\n",
    "        pct = count / len(model_df) * 100\n",
    "        print(f\"  {int(value)}: {description} (Count: {count:,}, {pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = model_df[feature_cols].copy()\n",
    "y = model_df[target_col].copy()\n",
    "\n",
    "# Convert to numeric if needed\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "    else:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "# Ensure target is numeric\n",
    "if y.dtype == 'object':\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y.astype(str))\n",
    "else:\n",
    "    y = pd.to_numeric(y, errors='coerce')\n",
    "\n",
    "# Final verification: ensure no missing indicators remain in target\n",
    "missing_indicators = [7, 9]  # GENHLTH missing indicators\n",
    "remaining_missing = y.isin(missing_indicators).sum() if hasattr(y, 'isin') else 0\n",
    "if remaining_missing > 0:\n",
    "    print(f\"WARNING: Found {remaining_missing} missing indicators still in target!\")\n",
    "else:\n",
    "    print(f\"✓ Confirmed: No missing indicators remain in target variable\")\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target classes: {np.unique(y)}\")\n",
    "print(f\"Class distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for class_val, count in zip(unique, counts):\n",
    "    print(f\"  Class {class_val}: {count:,} ({count/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Train Random Forest model\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Cross-validation score\n",
    "print(\"\\nPerforming 5-fold cross-validation...\")\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix with proper labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create proper class labels using GENHLTH metadata\n",
    "if genhlth_metadata and hasattr(genhlth_metadata, 'value_lookup'):\n",
    "    # Get unique values from the test set to know which classes are present\n",
    "    unique_classes = sorted(np.unique(np.concatenate([y_test, y_pred])))\n",
    "    class_labels = []\n",
    "    for class_val in unique_classes:\n",
    "        # For GENHLTH, the values are already meaningful (1=Excellent, 2=Very good, etc.)\n",
    "        # Since we didn't use label encoding (target is already numeric), use values directly\n",
    "        description = genhlth_metadata.value_lookup.get(int(class_val), f\"Code {class_val}\")\n",
    "        class_labels.append(description)\n",
    "else:\n",
    "    # Fallback to generic labels\n",
    "    class_labels = [f'Class {i}' for i in range(len(np.unique(y)))]\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix\\\\nPredicting General Health Status from Demographics')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Rankings:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['feature']:15s}: {row['importance']:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(min(15, len(feature_importance)))\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance\\n(Predicting General Health Status)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top features with their metadata\n",
    "print(\"Top 5 Most Important Features Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, (_, row) in enumerate(feature_importance.head(5).iterrows(), 1):\n",
    "    feature_name = row['feature']\n",
    "    importance = row['importance']\n",
    "    \n",
    "    print(f\"\\n{i}. {feature_name} (Importance: {importance:.4f})\")\n",
    "    \n",
    "    if feature_name in metadata:\n",
    "        meta = metadata[feature_name]\n",
    "        print(f\"   Label: {meta.label}\")\n",
    "        print(f\"   Question: {meta.question}\")\n",
    "        \n",
    "        # Show value distribution\n",
    "        if feature_name in model_df.columns:\n",
    "            value_counts = model_df[feature_name].value_counts().head(5)\n",
    "            print(f\"   Top values:\")\n",
    "            for val, count in value_counts.items():\n",
    "                desc = meta.value_lookup.get(val, f\"Code {val}\")\n",
    "                pct = count / len(model_df) * 100\n",
    "                print(f\"     {val}: {desc} ({count:,}, {pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   (No metadata available for calculated variable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: BRFSS 2023 with Semantic Null Conversion\")\n",
    "print(f\"Target Variable: {target_col} (General Health Status)\")\n",
    "print(f\"Number of Features: {len(feature_cols)}\")\n",
    "print(f\"Training Samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Test Samples: {X_test.shape[0]:,}\")\n",
    "\n",
    "print(f\"\\nSemantic Null Handling:\")\n",
    "print(f\"✓ Missing indicators (7=Don't know, 9=Refused) automatically converted to NaN\")\n",
    "print(f\"✓ Analysis focused on meaningful health status categories only\")\n",
    "print(f\"✓ Cleaner data preprocessing with fewer manual missing value checks\")\n",
    "\n",
    "print(f\"\\nModel: Random Forest Classifier\")\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "print(f\"\\nTop 3 Most Important Demographics:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(3).iterrows(), 1):\n",
    "    print(f\"  {i}. {row['feature']} (importance: {row['importance']:.3f})\")\n",
    "\n",
    "print(f\"\\nTarget Classes Used:\")\n",
    "target_counts = model_df[target_col].value_counts().sort_index()\n",
    "target_meta = metadata.get(target_col)\n",
    "for value, count in target_counts.items():\n",
    "    if target_meta:\n",
    "        description = target_meta.value_lookup.get(int(value), f\"Code {value}\")\n",
    "        print(f\"  {int(value)}: {description}\")\n",
    "\n",
    "print(\"\\nConclusion: The semantic null conversion feature simplifies data preprocessing\")\n",
    "print(\"and ensures consistent handling of missing/refused/unknown responses across\")\n",
    "print(\"the BRFSS dataset, leading to more reliable demographic health predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "print(\"Performing hyperparameter tuning...\")\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "    'min_samples_leaf': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Use smaller sample for grid search to speed up computation\n",
    "sample_size = min(50000, len(X_train))  # Increased sample size for better results\n",
    "print(f\"Using sample size of {sample_size:,} for hyperparameter tuning...\")\n",
    "\n",
    "X_sample = X_train.sample(n=sample_size, random_state=42)\n",
    "y_sample = y_train[X_sample.index]\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    cv=3,  # 3-fold CV to balance accuracy and speed\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting grid search (this may take several minutes)...\")\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "print(f\"\\nHyperparameter Tuning Results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Train final model with best parameters on full training set\n",
    "print(f\"\\nTraining optimized model on full training set...\")\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate tuned model\n",
    "best_y_pred = best_rf.predict(X_test)\n",
    "best_accuracy = accuracy_score(y_test, best_y_pred)\n",
    "\n",
    "print(f\"\\nModel Performance Comparison:\")\n",
    "print(f\"Original model accuracy: {accuracy:.3f}\")\n",
    "print(f\"Tuned model accuracy:    {best_accuracy:.3f}\")\n",
    "print(f\"Improvement: {(best_accuracy - accuracy):.3f} ({((best_accuracy - accuracy)/accuracy)*100:+.1f}%)\")\n",
    "\n",
    "# Show classification report for tuned model\n",
    "print(f\"\\nTuned Model Classification Report:\")\n",
    "print(classification_report(y_test, best_y_pred))\n",
    "\n",
    "# Compare confusion matrices: Original vs Tuned model\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Original model confusion matrix\n",
    "cm_original = confusion_matrix(y_test, y_pred)\n",
    "# Tuned model confusion matrix  \n",
    "cm_tuned = confusion_matrix(y_test, best_y_pred)\n",
    "\n",
    "# Create proper class labels using GENHLTH metadata\n",
    "if genhlth_metadata and hasattr(genhlth_metadata, 'value_lookup'):\n",
    "    unique_classes = sorted(np.unique(np.concatenate([y_test, y_pred])))\n",
    "    class_labels = []\n",
    "    for class_val in unique_classes:\n",
    "        description = genhlth_metadata.value_lookup.get(int(class_val), f\"Code {class_val}\")\n",
    "        class_labels.append(description)\n",
    "else:\n",
    "    class_labels = [f'Class {i}' for i in range(len(np.unique(y_test)))]\n",
    "\n",
    "# Plot original model confusion matrix\n",
    "sns.heatmap(cm_original, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels, yticklabels=class_labels, ax=ax1)\n",
    "ax1.set_title(f'Original Model\\nAccuracy: {accuracy:.3f}')\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "\n",
    "# Plot tuned model confusion matrix\n",
    "sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=class_labels, yticklabels=class_labels, ax=ax2)\n",
    "ax2.set_title(f'Tuned Model\\nAccuracy: {best_accuracy:.3f}')\n",
    "ax2.set_xlabel('Predicted')\n",
    "ax2.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importance comparison\n",
    "tuned_feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'original_importance': rf_model.feature_importances_,\n",
    "    'tuned_importance': best_rf.feature_importances_\n",
    "}).sort_values('tuned_importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Comparison (Top 10):\")\n",
    "print(\"=\" * 60)\n",
    "for i, (_, row) in enumerate(tuned_feature_importance.head(10).iterrows(), 1):\n",
    "    orig = row['original_importance']\n",
    "    tuned = row['tuned_importance'] \n",
    "    change = tuned - orig\n",
    "    print(f\"{i:2d}. {row['feature']:15s}: {orig:.4f} → {tuned:.4f} ({change:+.4f})\")\n",
    "\n",
    "# Plot feature importance comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = tuned_feature_importance.head(15)\n",
    "x = np.arange(len(top_features))\n",
    "width = 0.35\n",
    "\n",
    "plt.barh(x - width/2, top_features['original_importance'], width, \n",
    "         label='Original Model', alpha=0.8, color='skyblue')\n",
    "plt.barh(x + width/2, top_features['tuned_importance'], width,\n",
    "         label='Tuned Model', alpha=0.8, color='lightgreen')\n",
    "\n",
    "plt.yticks(x, top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importance: Original vs Tuned Model')\n",
    "plt.legend()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning with GridSearchCV\n",
    "print(\"Performing hyperparameter tuning...\")\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "    'min_samples_leaf': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Use smaller sample for grid search to speed up computation\n",
    "sample_size = min(50000, len(X_train))  # Increased sample size for better results\n",
    "print(f\"Using sample size of {sample_size:,} for hyperparameter tuning...\")\n",
    "\n",
    "X_sample = X_train.sample(n=sample_size, random_state=42)\n",
    "y_sample = y_train[X_sample.index]\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid,\n",
    "    cv=3,  # 3-fold CV to balance accuracy and speed\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting grid search (this may take several minutes)...\")\n",
    "grid_search.fit(X_sample, y_sample)\n",
    "\n",
    "print(f\"\\nHyperparameter Tuning Results:\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Train final model with best parameters on full training set\n",
    "print(f\"\\nTraining optimized model on full training set...\")\n",
    "best_rf = grid_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate tuned model\n",
    "best_y_pred = best_rf.predict(X_test)\n",
    "best_accuracy = accuracy_score(y_test, best_y_pred)\n",
    "\n",
    "print(f\"\\nModel Performance Comparison:\")\n",
    "print(f\"Original model accuracy: {accuracy:.3f}\")\n",
    "print(f\"Tuned model accuracy:    {best_accuracy:.3f}\")\n",
    "print(f\"Improvement: {(best_accuracy - accuracy):.3f} ({((best_accuracy - accuracy)/accuracy)*100:+.1f}%)\")\n",
    "\n",
    "# Show classification report for tuned model\n",
    "print(f\"\\nTuned Model Classification Report:\")\n",
    "print(classification_report(y_test, best_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
