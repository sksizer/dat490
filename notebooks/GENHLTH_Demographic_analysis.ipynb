{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENHLTH Demographic Analysis\n",
    "## Predicting General Health Status from Demographics using Random Forest\n",
    "\n",
    "This notebook analyzes the relationship between demographic variables and general health status (GENHLTH) in the BRFSS dataset using a Random Forest machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def is_colab():\n",
    "    return 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Setup for Google Colab environment only\n",
    "if is_colab():\n",
    "    GIT_REPO_URL = 'https://github.com/sksizer/dat490.git'\n",
    "    LOCAL_DIR = '/content/code/dat490'\n",
    "\n",
    "    if not os.path.exists(LOCAL_DIR):\n",
    "        print(f\"Cloning repo into {LOCAL_DIR}...\")\n",
    "        subprocess.run(['git', 'clone', GIT_REPO_URL, LOCAL_DIR], check=True)\n",
    "    else:\n",
    "        print(f\"Repo already exists at {LOCAL_DIR}, pulling latest changes...\")\n",
    "        subprocess.run(['git', '-C', LOCAL_DIR, 'pull'], check=True)\n",
    "\n",
    "    if LOCAL_DIR not in sys.path:\n",
    "        sys.path.insert(0, LOCAL_DIR)\n",
    "        print(f\"Added {LOCAL_DIR} to sys.path\")\n",
    "\n",
    "    # Import dat490 package\n",
    "    import dat490\n",
    "else:\n",
    "    # Running locally - assume dat490 is already available\n",
    "    import dat490"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine GENHLTH variable\n",
    "if 'GENHLTH' in df.columns:\n",
    "    genhlth_meta = metadata.get('GENHLTH')\n",
    "    if genhlth_meta:\n",
    "        print(f\"Column: {genhlth_meta.sas_variable_name}\")\n",
    "        print(f\"Label: {genhlth_meta.label}\")\n",
    "        print(f\"Question: {genhlth_meta.question}\")\n",
    "        print(f\"\\nValue mappings:\")\n",
    "        for value, description in genhlth_meta.value_lookup.items():\n",
    "            print(f\"  {value}: {description}\")\n",
    "    \n",
    "    # Show value counts\n",
    "    print(\"\\nGENHLTH Distribution:\")\n",
    "    genhlth_counts = df['GENHLTH'].value_counts().sort_index()\n",
    "    for value, count in genhlth_counts.items():\n",
    "        if not pd.isna(value) and genhlth_meta:\n",
    "            description = genhlth_meta.value_lookup.get(int(value), f\"Code {value}\")\n",
    "            print(f\"  {int(value)}: {description} (Count: {count:,})\")\n",
    "        else:\n",
    "            print(f\"  {value}: Missing (Count: {count:,})\")\n",
    "else:\n",
    "    print(\"GENHLTH column not found. Searching for similar health status variables...\")\n",
    "    health_cols = [col for col in df.columns if 'HLTH' in col or 'HEALTH' in col]\n",
    "    print(f\"Health-related columns found: {health_cols}\")\n",
    "\n",
    "# Store GENHLTH metadata for later use in confusion matrix\n",
    "genhlth_metadata = metadata.get('GENHLTH') if 'GENHLTH' in df.columns else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Target Variable: GENHLTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine GENHLTH variable\n",
    "if 'GENHLTH' in df.columns:\n",
    "    genhlth_meta = metadata.get('GENHLTH')\n",
    "    if genhlth_meta:\n",
    "        print(f\"Column: {genhlth_meta.sas_variable_name}\")\n",
    "        print(f\"Label: {genhlth_meta.label}\")\n",
    "        print(f\"Question: {genhlth_meta.question}\")\n",
    "        print(f\"\\nValue mappings:\")\n",
    "        for value, description in genhlth_meta.value_lookup.items():\n",
    "            print(f\"  {value}: {description}\")\n",
    "    \n",
    "    # Show value counts\n",
    "    print(\"\\nGENHLTH Distribution:\")\n",
    "    genhlth_counts = df['GENHLTH'].value_counts().sort_index()\n",
    "    for value, count in genhlth_counts.items():\n",
    "        if not pd.isna(value) and genhlth_meta:\n",
    "            description = genhlth_meta.value_lookup.get(int(value), f\"Code {value}\")\n",
    "            print(f\"  {int(value)}: {description} (Count: {count:,})\")\n",
    "        else:\n",
    "            print(f\"  {value}: Missing (Count: {count:,})\")\n",
    "else:\n",
    "    print(\"GENHLTH column not found. Searching for similar health status variables...\")\n",
    "    health_cols = [col for col in df.columns if 'HLTH' in col or 'HEALTH' in col]\n",
    "    print(f\"Health-related columns found: {health_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Demographics Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Demographics section features\n",
    "demographics_columns = [\n",
    "    col for col, meta in metadata.items()\n",
    "    if hasattr(meta, 'section_name') and 'Demographics' in meta.section_name\n",
    "]\n",
    "\n",
    "print(f\"Demographics columns ({len(demographics_columns)}): {demographics_columns}\")\n",
    "\n",
    "# Also look for calculated demographic variables\n",
    "calc_demo_columns = [\n",
    "    col for col in df.columns \n",
    "    if col.startswith('_') and any(demo in col.upper() for demo in ['AGE', 'RACE', 'SEX', 'INCOME', 'EDUC'])\n",
    "]\n",
    "\n",
    "print(f\"\\nCalculated demographic columns: {calc_demo_columns}\")\n",
    "\n",
    "# Combine and filter to available columns\n",
    "all_demo_features = list(set(demographics_columns + calc_demo_columns))\n",
    "available_demo_features = [col for col in all_demo_features if col in df.columns]\n",
    "\n",
    "print(f\"\\nFinal demographics features to use ({len(available_demo_features)}): {available_demo_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for key demographic variables\n",
    "key_demo_vars = ['MARITAL', 'EDUCA', 'EMPLOY1', 'INCOME3']\n",
    "available_key_vars = [var for var in key_demo_vars if var in df.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, var in enumerate(available_key_vars[:4]):\n",
    "    if var in metadata:\n",
    "        var_meta = metadata[var]\n",
    "        value_counts = df[var].value_counts().head(8)\n",
    "        \n",
    "        # Map values to descriptions\n",
    "        labels = []\n",
    "        for val in value_counts.index:\n",
    "            if not pd.isna(val):\n",
    "                desc = var_meta.value_lookup.get(int(val), f\"Code {val}\")\n",
    "                # Truncate long labels\n",
    "                if len(desc) > 30:\n",
    "                    desc = desc[:27] + \"...\"\n",
    "                labels.append(desc)\n",
    "            else:\n",
    "                labels.append(\"Missing\")\n",
    "        \n",
    "        axes[i].barh(range(len(labels)), value_counts.values)\n",
    "        axes[i].set_yticks(range(len(labels)))\n",
    "        axes[i].set_yticklabels(labels, fontsize=8)\n",
    "        axes[i].set_title(f\"{var}: {var_meta.label}\", fontsize=10)\n",
    "        axes[i].set_xlabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "target_col = 'GENHLTH'\n",
    "\n",
    "if target_col not in df.columns:\n",
    "    print(f\"Target variable {target_col} not found!\")\n",
    "    # Look for alternative health status variables\n",
    "    health_alternatives = [col for col in df.columns if 'HLTH' in col]\n",
    "    print(f\"Available health columns: {health_alternatives}\")\n",
    "    if health_alternatives:\n",
    "        target_col = health_alternatives[0]\n",
    "        print(f\"Using {target_col} as target variable instead\")\n",
    "    else:\n",
    "        raise ValueError(\"No suitable target variable found\")\n",
    "\n",
    "# Select demographics features that are available in the dataset\n",
    "feature_cols = [col for col in available_demo_features if col in df.columns and col != target_col]\n",
    "\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"Feature variables ({len(feature_cols)}): {feature_cols}\")\n",
    "\n",
    "# Create analysis dataset\n",
    "analysis_cols = [target_col] + feature_cols\n",
    "analysis_df = df[analysis_cols].copy()\n",
    "\n",
    "print(f\"\\nAnalysis dataset shape: {analysis_df.shape}\")\n",
    "print(f\"Missing values per column:\")\n",
    "missing_counts = analysis_df.isnull().sum()\n",
    "for col in analysis_cols:\n",
    "    missing_pct = missing_counts[col] / len(analysis_df) * 100\n",
    "    print(f\"  {col}: {missing_counts[col]:,} ({missing_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values and prepare for modeling\n",
    "# Drop rows with missing target variable\n",
    "model_df = analysis_df.dropna(subset=[target_col]).copy()\n",
    "print(f\"After dropping rows with missing target: {model_df.shape}\")\n",
    "\n",
    "# For features, we'll handle missing values by either dropping columns with too many missing values\n",
    "# or filling with mode for categorical variables\n",
    "missing_threshold = 0.3  # Drop columns with >30% missing\n",
    "\n",
    "cols_to_keep = []\n",
    "for col in feature_cols:\n",
    "    missing_pct = model_df[col].isnull().sum() / len(model_df)\n",
    "    if missing_pct <= missing_threshold:\n",
    "        cols_to_keep.append(col)\n",
    "    else:\n",
    "        print(f\"Dropping {col} due to {missing_pct:.1%} missing values\")\n",
    "\n",
    "feature_cols = cols_to_keep\n",
    "print(f\"\\nFeatures after missing value filtering: {feature_cols}\")\n",
    "\n",
    "# Fill remaining missing values with mode (most common value)\n",
    "for col in feature_cols:\n",
    "    if model_df[col].isnull().any():\n",
    "        mode_value = model_df[col].mode().iloc[0] if not model_df[col].mode().empty else 0\n",
    "        model_df[col] = model_df[col].fillna(mode_value)\n",
    "        print(f\"Filled {col} missing values with mode: {mode_value}\")\n",
    "\n",
    "print(f\"\\nFinal model dataset shape: {model_df.shape}\")\n",
    "print(f\"Missing values remaining: {model_df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = model_df[feature_cols].copy()\n",
    "y = model_df[target_col].copy()\n",
    "\n",
    "# Convert to numeric if needed\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "    else:\n",
    "        X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "# Ensure target is numeric\n",
    "if y.dtype == 'object':\n",
    "    le_target = LabelEncoder()\n",
    "    y = le_target.fit_transform(y.astype(str))\n",
    "else:\n",
    "    y = pd.to_numeric(y, errors='coerce')\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target classes: {np.unique(y)}\")\n",
    "print(f\"Class distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for class_val, count in zip(unique, counts):\n",
    "    print(f\"  Class {class_val}: {count:,} ({count/len(y)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "\n",
    "# Train Random Forest model\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Cross-validation score\n",
    "print(\"\\nPerforming 5-fold cross-validation...\")\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"CV Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix with proper labels\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create proper class labels using GENHLTH metadata\n",
    "if genhlth_metadata and hasattr(genhlth_metadata, 'value_lookup'):\n",
    "    # Get unique values from the test set to know which classes are present\n",
    "    unique_classes = sorted(np.unique(np.concatenate([y_test, y_pred])))\n",
    "    class_labels = []\n",
    "    for class_val in unique_classes:\n",
    "        # For GENHLTH, the values are already meaningful (1=Excellent, 2=Very good, etc.)\n",
    "        # Since we didn't use label encoding (target is already numeric), use values directly\n",
    "        description = genhlth_metadata.value_lookup.get(int(class_val), f\"Code {class_val}\")\n",
    "        class_labels.append(description)\n",
    "else:\n",
    "    # Fallback to generic labels\n",
    "    class_labels = [f'Class {i}' for i in range(len(np.unique(y)))]\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_labels,\n",
    "            yticklabels=class_labels)\n",
    "plt.title('Confusion Matrix\\\\nPredicting General Health Status from Demographics')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Rankings:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['feature']:15s}: {row['importance']:.4f}\")\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = feature_importance.head(min(15, len(feature_importance)))\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest Feature Importance\\n(Predicting General Health Status)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze top features with their metadata\n",
    "print(\"Top 5 Most Important Features Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i, (_, row) in enumerate(feature_importance.head(5).iterrows(), 1):\n",
    "    feature_name = row['feature']\n",
    "    importance = row['importance']\n",
    "    \n",
    "    print(f\"\\n{i}. {feature_name} (Importance: {importance:.4f})\")\n",
    "    \n",
    "    if feature_name in metadata:\n",
    "        meta = metadata[feature_name]\n",
    "        print(f\"   Label: {meta.label}\")\n",
    "        print(f\"   Question: {meta.question}\")\n",
    "        \n",
    "        # Show value distribution\n",
    "        if feature_name in model_df.columns:\n",
    "            value_counts = model_df[feature_name].value_counts().head(5)\n",
    "            print(f\"   Top values:\")\n",
    "            for val, count in value_counts.items():\n",
    "                desc = meta.value_lookup.get(val, f\"Code {val}\")\n",
    "                pct = count / len(model_df) * 100\n",
    "                print(f\"     {val}: {desc} ({count:,}, {pct:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   (No metadata available for calculated variable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: BRFSS 2023\")\n",
    "print(f\"Target Variable: {target_col} (General Health Status)\")\n",
    "print(f\"Number of Features: {len(feature_cols)}\")\n",
    "print(f\"Training Samples: {X_train.shape[0]:,}\")\n",
    "print(f\"Test Samples: {X_test.shape[0]:,}\")\n",
    "print(f\"\\nModel: Random Forest Classifier\")\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "print(f\"\\nTop 3 Most Important Demographics:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head(3).iterrows(), 1):\n",
    "    print(f\"  {i}. {row['feature']} (importance: {row['importance']:.3f})\")\n",
    "print(\"\\nConclusion: Demographics show predictive power for general health status,\")\n",
    "print(\"with the most important factors being the top-ranked features above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Hyperparameter tuning with GridSearchCV\n",
    "# Uncomment and run this cell for better model performance\n",
    "\n",
    "# print(\"Performing hyperparameter tuning...\")\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 200],\n",
    "#     'max_depth': [5, 10, 15, None],\n",
    "#     'min_samples_split': [5, 10, 20],\n",
    "#     'min_samples_leaf': [2, 5, 10]\n",
    "# }\n",
    "\n",
    "# # Use smaller sample for grid search to speed up\n",
    "# sample_size = min(10000, len(X_train))\n",
    "# X_sample = X_train.sample(n=sample_size, random_state=42)\n",
    "# y_sample = y_train[X_sample.index]\n",
    "\n",
    "# grid_search = GridSearchCV(\n",
    "#     RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "#     param_grid,\n",
    "#     cv=3,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# grid_search.fit(X_sample, y_sample)\n",
    "# print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best CV score: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# # Train final model with best parameters\n",
    "# best_rf = grid_search.best_estimator_\n",
    "# best_rf.fit(X_train, y_train)\n",
    "# best_accuracy = accuracy_score(y_test, best_rf.predict(X_test))\n",
    "# print(f\"Tuned model test accuracy: {best_accuracy:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
