{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Goal\n",
    "## Original Questions\n",
    "- Can we use Chronic Health Conditions to accurately predict Health Care Access?\n",
    "- Are there Demographic clusters that are disproportionately affected by Chronic Health Conditions?\n",
    "- Can unsupervised learning methods reveal distinct clusters that account for the bulk of Chronic Health Conditions?\n",
    "\n",
    "### Questions:\n",
    "- I have gotten a bit hung up bc as worded 2 and 3 seem to be asking same thing?\n",
    "- were there established chronic health conditions?\n",
    "- established demographic features?\n",
    "- what are the features used in RQ1, and RQ3?\n",
    "\n",
    "#### Rough Plan:\n",
    "Question: Refinement:\n",
    "Are there diagnostically useful demographic clusters that indicate chronic health conditions?\n",
    "- Are there demographic clusters that strongly indicate certain chronic health conditions?\n",
    "- Can we predict chronic health conditions from demographics, and how does a ML model compare with simpler cluster membership?\n",
    "\n",
    "##### Part 1: Clustering\n",
    "- Cluster the demographic features of the BRFSS data\n",
    "- Visualize clusters and prevalence of chronic health conditions within each cluster\n",
    "    - what chronic health conditions to use?\n",
    "    - VISUAL: Clustering results\n",
    "    - VISUAL:  Heatmaps of cluster membership vs chronic health conditions\n",
    "- Run statistical tests to determine if certain clusters are significantly more affected by chronic health conditions\n",
    "    - Translation - test the strength of correlation between cluster membership and chronic health conditions\n",
    "    - Does being a member of a cluster correlate with having a chronic health condition?\n",
    "#### Part 2: Prediction of Chronic Health Conditions From Demographics via DL Model. Inversion of Question 3\n",
    "- use cluster labels as features?\n",
    "- compare performance of a deep learning model vs simpler clustering membership\n",
    "\n",
    "\n",
    "\n",
    "### Misc\n",
    "- potential 'linchpin' variables given we are clustering on demographics (and ran random forest on demographics)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "from IPython import get_ipython\n",
    "import logging\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import kmodes\n",
    "    print(\"kmodes already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing kmodes...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"kmodes\"])\n",
    "    import kmodes\n",
    "    print(\"kmodes installed successfully\")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def is_colab():\n",
    "    return 'google.colab' in str(get_ipython())\n",
    "\n",
    "# Set up environment and paths\n",
    "if is_colab():\n",
    "    print(\"Running in Google Colab\")\n",
    "\n",
    "    # Clone the repository if not already cloned\n",
    "    if not os.path.exists('dat490'):\n",
    "        import subprocess\n",
    "        print(\"Cloning repository...\")\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/sksizer/dat490.git'], check=True)\n",
    "        print(\"Repository cloned successfully\")\n",
    "\n",
    "    # Add the repository to Python path for imports\n",
    "    sys.path.insert(0, '/content/dat490')\n",
    "\n",
    "    # Set paths to use data from the cloned repository\n",
    "    BFRSS_DATA_PATH = 'dat490/data/LLCP2023.parquet'\n",
    "    BFRSS_CODEBOOK_PATH = 'dat490/data/codebook_USCODE23_LLCP_021924.HTML'\n",
    "    BFRSS_DESC_PATH = 'dat490/data/LLCP2023_desc.parquet'  # Additional metadata file if needed\n",
    "else:\n",
    "    print(\"Running in local environment\")\n",
    "\n",
    "    # Add parent directory to path for dat490 module imports\n",
    "    sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "    # Use local data paths\n",
    "    BFRSS_DATA_PATH = '../data/LLCP2023.parquet'\n",
    "    BFRSS_CODEBOOK_PATH = '../data/codebook_USCODE23_LLCP_021924.HTML'\n",
    "    BFRSS_DESC_PATH = '../data/LLCP2023_desc.parquet'  # Additional metadata file if needed\n",
    "\n",
    "# Verify files exist\n",
    "print(f\"\\\\nData path: {BFRSS_DATA_PATH}\")\n",
    "print(f\"Codebook path: {BFRSS_CODEBOOK_PATH}\")\n",
    "\n",
    "if not os.path.exists(BFRSS_DATA_PATH):\n",
    "    raise FileNotFoundError(f\"Data file not found at {BFRSS_DATA_PATH}\")\n",
    "\n",
    "if not os.path.exists(BFRSS_CODEBOOK_PATH):\n",
    "    raise FileNotFoundError(f\"Codebook file not found at {BFRSS_CODEBOOK_PATH}\")\n",
    "\n",
    "print(\"\\\\nAll required files found!\")\n",
    "logger.info('Environment setup complete')\n",
    "\n",
    "##################################\n",
    "# Load BFRSS data and metadata using the new wrapper\n",
    "from dat490 import load_bfrss\n",
    "\n",
    "# Single function call to load everything\n",
    "# exclude_desc_columns=True will exclude _DESC columns from metadata generation\n",
    "bfrss = load_bfrss(exclude_desc_columns=True)\n",
    "\n",
    "# Get a copy of the raw DataFrame\n",
    "bfrss_raw_df = bfrss.cloneDF()\n",
    "bfrss_raw_df.info()\n",
    "\n",
    "DEMOGRAPHIC_FEATURE_COLUMNS = [\n",
    "    # Demographics section columns (13 total)\n",
    "    # Demographics section columns (13 total)\n",
    "    'MARITAL',    # https://singular-eclair-6a5a16.netlify.app/columns/MARITAL\n",
    "    'EDUCA',      # https://singular-eclair-6a5a16.netlify.app/columns/EDUCA\n",
    "    'RENTHOM1',   # https://singular-eclair-6a5a16.netlify.app/columns/RENTHOM1\n",
    "    # 'NUMHHOL4',   # https://singular-eclair-6a5a16.netlify.app/columns/NUMHHOL4\n",
    "    # 'NUMPHON4',   # https://singular-eclair-6a5a16.netlify.app/columns/NUMPHON4\n",
    "    # 'CPDEMO1C',   # https://singular-eclair-6a5a16.netlify.app/columns/CPDEMO1C\n",
    "    'VETERAN3',   # https://singular-eclair-6a5a16.netlify.app/columns/VETERAN3\n",
    "    'EMPLOY1',    # https://singular-eclair-6a5a16.netlify.app/columns/EMPLOY1\n",
    "    # 'CHILDREN',   # https://singular-eclair-6a5a16.netlify.app/columns/CHILDREN\n",
    "    'INCOME3',    # https://singular-eclair-6a5a16.netlify.app/columns/INCOME3\n",
    "    # 'PREGNANT',   # https://singular-eclair-6a5a16.netlify.app/columns/PREGNANT\n",
    "    'SEXVAR',    # https://singular-eclair-6a5a16.netlify.app/columns/SEXVAR\n",
    "    '_HISPANC', # https://singular-eclair-6a5a16.netlify.app/columns/_HISPANC # Calculated but not sure from what\n",
    "    # '_CRACE1',    # https://singular-eclair-6a5a16.netlify.app/columns/_CRACE1 # Child race\n",
    "    '_IMPRACE',   # https://singular-eclair-6a5a16.netlify.app/columns/_IMPRACE\n",
    "    # '_AGE80',     # https://singular-eclair-6a5a16.netlify.app/columns/_AGE80\n",
    "]\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# K-Modes Analysis of BRFSS Data\n",
    "\n",
    "K-Modes clustering is an extension of K-Means designed for categorical data. Instead of using means to define cluster centers, K-Modes uses modes (most frequent values) and measures dissimilarity using the number of mismatches between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from kmodes.kmodes import KModes\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from kmodes.kmodes import KModes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def kmode_analysis(df: pd.DataFrame, feature_cols: list, max_clusters: int = 6):\n",
    "    data = df[feature_cols].copy()\n",
    "\n",
    "    # Encode categorical variables\n",
    "    encoders = {}\n",
    "    for col in data.columns:\n",
    "        if data[col].dtype == 'object' or data[col].dtype.name == 'category':\n",
    "            le = LabelEncoder()\n",
    "            data[col] = data[col].fillna(data[col].mode()[0])\n",
    "            data[col] = le.fit_transform(data[col])\n",
    "            encoders[col] = le\n",
    "        else:\n",
    "            data[col] = data[col].fillna(data[col].mean())\n",
    "\n",
    "    # Elbow Method\n",
    "    cost = []\n",
    "    for k in range(1, max_clusters + 1):\n",
    "        km = KModes(n_clusters=k, init='Huang', n_init=5, verbose=0)\n",
    "        km.fit_predict(data)\n",
    "        cost.append(km.cost_)\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, max_clusters + 1), cost, marker='o')\n",
    "    plt.title('Elbow Method - Cost vs. Number of Clusters')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Fit the best model (you can tune n_clusters here if needed)\n",
    "    optimal_k = 2  # or set based on elbow result\n",
    "    km = KModes(n_clusters=optimal_k, init='Huang', n_init=5, verbose=0)\n",
    "    clusters = km.fit_predict(data)\n",
    "    df['Cluster'] = clusters\n",
    "\n",
    "    # Cluster sizes\n",
    "    cluster_counts = df['Cluster'].value_counts().sort_index()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.barplot(x=cluster_counts.index, y=cluster_counts.values)\n",
    "    plt.title('Cluster Size Distribution')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Number of Records')\n",
    "    plt.show()\n",
    "\n",
    "    # Cluster composition heatmap\n",
    "    cluster_profiles = pd.DataFrame(data)\n",
    "    cluster_profiles['Cluster'] = clusters\n",
    "    summary = cluster_profiles.groupby('Cluster')[feature_cols].agg(lambda x: x.value_counts().index[0])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(summary.applymap(float), annot=True, fmt=\".0f\", cmap=\"YlGnBu\")\n",
    "    plt.title(\"Cluster Centroids (Encoded)\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Cluster\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print centroids and return DataFrame\n",
    "    print(\"Cluster centroids:\")\n",
    "    print(km.cluster_centroids_)\n",
    "    return df\n",
    "\n",
    "kmode_analysis(bfrss_raw_df, DEMOGRAPHIC_FEATURE_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Machine Learn Model to Predict Chronic Health Conditions from Demographics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "lwtdiqyh467",
   "source": "# Data preprocessing function\ndef preprocess_data_for_ml(df, feature_cols, target_col, encode_target=True):\n    \"\"\"\n    Preprocess data for machine learning:\n    - Handle missing values\n    - Encode categorical variables\n    - Scale numerical features\n    \n    Returns:\n    - X: Preprocessed features\n    - y: Target variable\n    - encoders: Dictionary of label encoders for categorical features\n    - scaler: StandardScaler object\n    \"\"\"\n    # Create a copy of the dataframe\n    data = df.copy()\n    \n    # Filter to only include rows with valid target values\n    if encode_target:\n        # For categorical targets, filter out missing/refused/don't know responses\n        valid_mask = data[target_col].notna()\n        if data[target_col].dtype == 'object' or data[target_col].dtype.name == 'category':\n            # Common invalid responses in BRFSS data\n            invalid_responses = ['7', '9', 'Dont know', 'Refused', 'Missing']\n            valid_mask = valid_mask & (~data[target_col].astype(str).isin(invalid_responses))\n    else:\n        valid_mask = data[target_col].notna()\n    \n    data = data[valid_mask]\n    \n    # Prepare features\n    X = data[feature_cols].copy()\n    y = data[target_col].copy()\n    \n    # Dictionary to store encoders\n    encoders = {}\n    \n    # Encode categorical features\n    for col in X.columns:\n        if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n            le = LabelEncoder()\n            # Fill missing values with mode\n            mode_val = X[col].mode()[0] if len(X[col].mode()) > 0 else 'missing'\n            X[col] = X[col].fillna(mode_val)\n            X[col] = le.fit_transform(X[col].astype(str))\n            encoders[col] = le\n        else:\n            # Fill missing numerical values with mean\n            X[col] = X[col].fillna(X[col].mean())\n    \n    # Encode target variable if needed\n    if encode_target and (y.dtype == 'object' or y.dtype.name == 'category'):\n        target_encoder = LabelEncoder()\n        y = target_encoder.fit_transform(y.astype(str))\n        encoders['target'] = target_encoder\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n    \n    return X_scaled, y, encoders, scaler\n\n# Function to create a binary target from chronic condition responses\ndef create_binary_target(series, positive_values=['1', 'Yes']):\n    \"\"\"\n    Convert BRFSS response to binary (0/1) target.\n    Typically, '1' or 'Yes' means the person has the condition.\n    \"\"\"\n    return series.astype(str).isin([str(v) for v in positive_values]).astype(int)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5xukjne9qd3",
   "source": "# Install TensorFlow if needed\ntry:\n    import tensorflow as tf\n    print(f\"TensorFlow version: {tf.__version__}\")\nexcept ImportError:\n    print(\"Installing TensorFlow...\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tensorflow\"])\n    import tensorflow as tf\n    print(f\"TensorFlow installed successfully. Version: {tf.__version__}\")\n\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nimport numpy as np",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Subsampling and Stability Testing for K-Modes\n\nimport numpy as np\nfrom sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\nfrom collections import defaultdict\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndef subsample_kmode_stability(df: pd.DataFrame, \n                            feature_cols: list, \n                            n_clusters: int = 2,\n                            n_iterations: int = 10,\n                            subsample_ratio: float = 0.8,\n                            random_state: int = 42):\n    \"\"\"\n    Perform stability testing of k-modes clustering through subsampling.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Input dataframe\n    feature_cols : list\n        List of feature columns to use for clustering\n    n_clusters : int\n        Number of clusters for k-modes\n    n_iterations : int\n        Number of subsampling iterations\n    subsample_ratio : float\n        Proportion of data to sample in each iteration\n    random_state : int\n        Random seed for reproducibility\n        \n    Returns:\n    --------\n    dict : Dictionary containing stability metrics and results\n    \"\"\"\n    \n    np.random.seed(random_state)\n    \n    # Prepare data\n    data = df[feature_cols].copy()\n    \n    # Encode categorical variables\n    encoders = {}\n    for col in data.columns:\n        if data[col].dtype == 'object' or data[col].dtype.name == 'category':\n            le = LabelEncoder()\n            data[col] = data[col].fillna(data[col].mode()[0] if len(data[col].mode()) > 0 else 'missing')\n            data[col] = le.fit_transform(data[col])\n            encoders[col] = le\n        else:\n            data[col] = data[col].fillna(data[col].mean())\n    \n    # Storage for results\n    results = {\n        'cluster_assignments': [],\n        'centroids': [],\n        'costs': [],\n        'ari_scores': [],\n        'nmi_scores': [],\n        'stability_scores': []\n    }\n    \n    # Reference clustering on full dataset\n    km_ref = KModes(n_clusters=n_clusters, init='Huang', n_init=5, verbose=0)\n    ref_clusters = km_ref.fit_predict(data)\n    ref_centroids = km_ref.cluster_centroids_\n    \n    print(f\"Running {n_iterations} iterations with {subsample_ratio*100:.0f}% subsampling...\")\n    \n    for i in range(n_iterations):\n        # Subsample data\n        n_samples = int(len(data) * subsample_ratio)\n        sample_idx = np.random.choice(len(data), n_samples, replace=False)\n        data_sample = data.iloc[sample_idx]\n        \n        # Run k-modes on subsample\n        km = KModes(n_clusters=n_clusters, init='Huang', n_init=5, verbose=0)\n        clusters = km.fit_predict(data_sample)\n        \n        # Store results\n        results['cluster_assignments'].append(clusters)\n        results['centroids'].append(km.cluster_centroids_)\n        results['costs'].append(km.cost_)\n        \n        # Calculate stability metrics against reference (for overlapping samples)\n        ref_clusters_sample = ref_clusters[sample_idx]\n        ari = adjusted_rand_score(ref_clusters_sample, clusters)\n        nmi = normalized_mutual_info_score(ref_clusters_sample, clusters)\n        \n        results['ari_scores'].append(ari)\n        results['nmi_scores'].append(nmi)\n        \n        if (i + 1) % 5 == 0:\n            print(f\"  Completed iteration {i+1}/{n_iterations}\")\n    \n    # Calculate pairwise stability between iterations\n    print(\"\\\\nCalculating pairwise stability...\")\n    for i in range(n_iterations):\n        for j in range(i+1, n_iterations):\n            # Find common indices between two subsamples\n            # Since we're using random sampling, we need to track indices\n            # For simplicity, we'll calculate stability on the full dataset predictions\n            pass\n    \n    # Summary statistics\n    results['summary'] = {\n        'mean_cost': np.mean(results['costs']),\n        'std_cost': np.std(results['costs']),\n        'mean_ari': np.mean(results['ari_scores']),\n        'std_ari': np.std(results['ari_scores']),\n        'mean_nmi': np.mean(results['nmi_scores']),\n        'std_nmi': np.std(results['nmi_scores']),\n        'reference_centroids': ref_centroids,\n        'reference_clusters': ref_clusters\n    }\n    \n    return results\n\n# Run stability analysis\nstability_results = subsample_kmode_stability(\n    bfrss_raw_df, \n    DEMOGRAPHIC_FEATURE_COLUMNS,\n    n_clusters=2,\n    n_iterations=20,\n    subsample_ratio=0.8\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "id": "e62dd3da7159f5f4"
  },
  {
   "cell_type": "code",
   "source": "# Advanced Stability Metrics with Pairwise Comparisons\n\ndef calculate_pairwise_stability(df: pd.DataFrame,\n                               feature_cols: list,\n                               n_clusters: int = 2,\n                               n_iterations: int = 10,\n                               subsample_ratio: float = 0.8,\n                               random_state: int = 42):\n    \"\"\"\n    Calculate pairwise stability between multiple k-modes runs with bootstrap confidence intervals.\n    \"\"\"\n    \n    np.random.seed(random_state)\n    \n    # Prepare data\n    data = df[feature_cols].copy()\n    \n    # Encode categorical variables\n    for col in data.columns:\n        if data[col].dtype == 'object' or data[col].dtype.name == 'category':\n            le = LabelEncoder()\n            data[col] = data[col].fillna(data[col].mode()[0] if len(data[col].mode()) > 0 else 'missing')\n            data[col] = le.fit_transform(data[col])\n        else:\n            data[col] = data[col].fillna(data[col].mean())\n    \n    # Store sample indices and cluster assignments\n    sample_indices = []\n    cluster_assignments = []\n    centroids = []\n    \n    print(f\"Running {n_iterations} k-modes iterations...\")\n    \n    for i in range(n_iterations):\n        # Subsample data\n        n_samples = int(len(data) * subsample_ratio)\n        sample_idx = np.random.choice(len(data), n_samples, replace=False)\n        sample_indices.append(sample_idx)\n        \n        data_sample = data.iloc[sample_idx]\n        \n        # Run k-modes\n        km = KModes(n_clusters=n_clusters, init='Huang', n_init=5, verbose=0)\n        clusters = km.fit_predict(data_sample)\n        \n        cluster_assignments.append(clusters)\n        centroids.append(km.cluster_centroids_)\n        \n        if (i + 1) % 5 == 0:\n            print(f\"  Completed iteration {i+1}/{n_iterations}\")\n    \n    # Calculate pairwise stability\n    print(\"\\\\nCalculating pairwise stability metrics...\")\n    pairwise_ari = np.zeros((n_iterations, n_iterations))\n    pairwise_nmi = np.zeros((n_iterations, n_iterations))\n    \n    for i in range(n_iterations):\n        for j in range(i, n_iterations):\n            if i == j:\n                pairwise_ari[i, j] = 1.0\n                pairwise_nmi[i, j] = 1.0\n            else:\n                # Find common indices\n                common_idx = np.intersect1d(sample_indices[i], sample_indices[j])\n                \n                if len(common_idx) > 0:\n                    # Get positions in respective samples\n                    pos_i = np.searchsorted(sample_indices[i], common_idx)\n                    pos_j = np.searchsorted(sample_indices[j], common_idx)\n                    \n                    # Compare cluster assignments for common samples\n                    clusters_i = cluster_assignments[i][pos_i]\n                    clusters_j = cluster_assignments[j][pos_j]\n                    \n                    ari = adjusted_rand_score(clusters_i, clusters_j)\n                    nmi = normalized_mutual_info_score(clusters_i, clusters_j)\n                    \n                    pairwise_ari[i, j] = pairwise_ari[j, i] = ari\n                    pairwise_nmi[i, j] = pairwise_nmi[j, i] = nmi\n    \n    # Calculate centroid stability\n    centroid_distances = np.zeros((n_iterations, n_iterations))\n    for i in range(n_iterations):\n        for j in range(i, n_iterations):\n            if i == j:\n                centroid_distances[i, j] = 0.0\n            else:\n                # Calculate Hamming distance between centroids\n                dist = 0\n                for k in range(n_clusters):\n                    dist += np.sum(centroids[i][k] != centroids[j][k])\n                centroid_distances[i, j] = centroid_distances[j, i] = dist / (n_clusters * len(feature_cols))\n    \n    return {\n        'pairwise_ari': pairwise_ari,\n        'pairwise_nmi': pairwise_nmi,\n        'centroid_distances': centroid_distances,\n        'mean_ari': np.mean(pairwise_ari[np.triu_indices(n_iterations, k=1)]),\n        'std_ari': np.std(pairwise_ari[np.triu_indices(n_iterations, k=1)]),\n        'mean_nmi': np.mean(pairwise_nmi[np.triu_indices(n_iterations, k=1)]),\n        'std_nmi': np.std(pairwise_nmi[np.triu_indices(n_iterations, k=1)]),\n        'mean_centroid_dist': np.mean(centroid_distances[np.triu_indices(n_iterations, k=1)]),\n        'std_centroid_dist': np.std(centroid_distances[np.triu_indices(n_iterations, k=1)])\n    }\n\n# Run pairwise stability analysis\npairwise_results = calculate_pairwise_stability(\n    bfrss_raw_df,\n    DEMOGRAPHIC_FEATURE_COLUMNS,\n    n_clusters=2,\n    n_iterations=15,\n    subsample_ratio=0.8\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "id": "9bbe4f7ff0fe3111"
  },
  {
   "cell_type": "code",
   "source": "# Visualization Functions for Stability Results\n\ndef visualize_stability_results(results, title_prefix=\"\"):\n    \"\"\"\n    Create comprehensive visualizations for k-modes stability analysis.\n    \"\"\"\n    \n    # Set up the figure with subplots\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    fig.suptitle(f'{title_prefix}K-Modes Clustering Stability Analysis', fontsize=16)\n    \n    # 1. Stability Metrics Over Iterations (if from subsample_kmode_stability)\n    if 'ari_scores' in results:\n        ax = axes[0, 0]\n        iterations = range(1, len(results['ari_scores']) + 1)\n        \n        ax.plot(iterations, results['ari_scores'], 'o-', label='ARI', markersize=8)\n        ax.plot(iterations, results['nmi_scores'], 's-', label='NMI', markersize=8)\n        \n        # Add mean lines\n        ax.axhline(y=results['summary']['mean_ari'], color='blue', linestyle='--', alpha=0.5)\n        ax.axhline(y=results['summary']['mean_nmi'], color='orange', linestyle='--', alpha=0.5)\n        \n        ax.set_xlabel('Iteration')\n        ax.set_ylabel('Score')\n        ax.set_title('Stability Metrics vs Reference Clustering')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax.set_ylim(0, 1.05)\n    \n    # 2. Pairwise Stability Heatmap (if from calculate_pairwise_stability)\n    if 'pairwise_ari' in results:\n        ax = axes[0, 1]\n        sns.heatmap(results['pairwise_ari'], annot=True, fmt='.2f', cmap='YlOrRd', \n                   vmin=0, vmax=1, square=True, ax=ax, cbar_kws={'label': 'ARI'})\n        ax.set_title('Pairwise ARI Between Iterations')\n        ax.set_xlabel('Iteration')\n        ax.set_ylabel('Iteration')\n    \n    # 3. Distribution of Stability Scores\n    if 'pairwise_ari' in results:\n        ax = axes[1, 0]\n        \n        # Extract upper triangle values (excluding diagonal)\n        n_iter = results['pairwise_ari'].shape[0]\n        ari_values = results['pairwise_ari'][np.triu_indices(n_iter, k=1)]\n        nmi_values = results['pairwise_nmi'][np.triu_indices(n_iter, k=1)]\n        \n        # Create violin plots\n        data_to_plot = [ari_values, nmi_values]\n        positions = [1, 2]\n        \n        parts = ax.violinplot(data_to_plot, positions=positions, showmeans=True, showmedians=True)\n        \n        # Customize colors\n        colors = ['#3498db', '#e74c3c']\n        for pc, color in zip(parts['bodies'], colors):\n            pc.set_facecolor(color)\n            pc.set_alpha(0.7)\n        \n        ax.set_xticks(positions)\n        ax.set_xticklabels(['ARI', 'NMI'])\n        ax.set_ylabel('Score')\n        ax.set_title('Distribution of Pairwise Stability Scores')\n        ax.set_ylim(0, 1.05)\n        ax.grid(True, axis='y', alpha=0.3)\n        \n        # Add summary statistics\n        ax.text(1, 0.05, f'μ={np.mean(ari_values):.3f}\\\\nσ={np.std(ari_values):.3f}', \n                ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n        ax.text(2, 0.05, f'μ={np.mean(nmi_values):.3f}\\\\nσ={np.std(nmi_values):.3f}', \n                ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    \n    # 4. Centroid Stability\n    if 'centroid_distances' in results:\n        ax = axes[1, 1]\n        \n        # Create a histogram of centroid distances\n        distances = results['centroid_distances'][np.triu_indices(results['centroid_distances'].shape[0], k=1)]\n        \n        ax.hist(distances, bins=20, edgecolor='black', alpha=0.7)\n        ax.set_xlabel('Normalized Hamming Distance')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Distribution of Centroid Distances')\n        ax.grid(True, axis='y', alpha=0.3)\n        \n        # Add mean line\n        mean_dist = np.mean(distances)\n        ax.axvline(x=mean_dist, color='red', linestyle='--', linewidth=2, \n                  label=f'Mean = {mean_dist:.3f}')\n        ax.legend()\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    # Print summary statistics\n    print(\"\\\\n=== Stability Analysis Summary ===\")\n    if 'summary' in results:\n        print(f\"Mean ARI: {results['summary']['mean_ari']:.3f} ± {results['summary']['std_ari']:.3f}\")\n        print(f\"Mean NMI: {results['summary']['mean_nmi']:.3f} ± {results['summary']['std_nmi']:.3f}\")\n    elif 'mean_ari' in results:\n        print(f\"Mean Pairwise ARI: {results['mean_ari']:.3f} ± {results['std_ari']:.3f}\")\n        print(f\"Mean Pairwise NMI: {results['mean_nmi']:.3f} ± {results['std_nmi']:.3f}\")\n        print(f\"Mean Centroid Distance: {results['mean_centroid_dist']:.3f} ± {results['std_centroid_dist']:.3f}\")\n    \n    return fig\n\n# Visualize the stability results\nif 'stability_results' in locals():\n    visualize_stability_results(stability_results, \"Subsample \")\n    \nif 'pairwise_results' in locals():\n    visualize_stability_results(pairwise_results, \"Pairwise \")",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "id": "2e1942ceada33750"
  },
  {
   "cell_type": "code",
   "source": "# Quick Proof of Concept with Small Subsampling\n\ndef quick_stability_test(df: pd.DataFrame,\n                        feature_cols: list,\n                        n_clusters: int = 2,\n                        n_iterations: int = 5,\n                        subsample_size: int = 1000,  # Absolute sample size instead of ratio\n                        random_state: int = 42):\n    \"\"\"\n    Quick stability test with small absolute sample sizes for proof of concept.\n    \n    Parameters:\n    -----------\n    subsample_size : int\n        Absolute number of samples to use (default 1000 for quick testing)\n    \"\"\"\n    \n    # Calculate the ratio based on absolute size\n    total_rows = len(df)\n    subsample_ratio = min(subsample_size / total_rows, 1.0)\n    \n    print(f\"\\\\n=== Quick Stability Test ===\")\n    print(f\"Total rows: {total_rows:,}\")\n    print(f\"Sample size: {subsample_size:,} ({subsample_ratio*100:.1f}% of data)\")\n    print(f\"Iterations: {n_iterations}\")\n    print(f\"Clusters: {n_clusters}\\\\n\")\n    \n    # Run the pairwise stability analysis with small samples\n    results = calculate_pairwise_stability(\n        df,\n        feature_cols,\n        n_clusters=n_clusters,\n        n_iterations=n_iterations,\n        subsample_ratio=subsample_ratio,\n        random_state=random_state\n    )\n    \n    return results\n\n# Run quick test with very small sample\nprint(\"Running quick proof of concept with 1000 samples...\")\nquick_results = quick_stability_test(\n    bfrss_raw_df,\n    DEMOGRAPHIC_FEATURE_COLUMNS,\n    n_clusters=2,\n    n_iterations=5,\n    subsample_size=1000  # Only 1000 samples for quick testing\n)\n\n# Visualize quick results\nvisualize_stability_results(quick_results, \"Quick Test - \")",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "id": "3b8ab403b6934e9e"
  },
  {
   "cell_type": "code",
   "source": "# Comprehensive Stability Analysis with Multiple Sample Sizes\n\ndef multi_scale_stability_analysis(df: pd.DataFrame,\n                                 feature_cols: list,\n                                 n_clusters: int = 2,\n                                 sample_sizes: list = [500, 1000, 5000, 10000],\n                                 n_iterations: int = 10,\n                                 random_state: int = 42):\n    \"\"\"\n    Run stability analysis across multiple sample sizes to understand scaling behavior.\n    \"\"\"\n    \n    results_by_size = {}\n    \n    for sample_size in sample_sizes:\n        print(f\"\\\\n{'='*50}\")\n        print(f\"Testing with sample size: {sample_size:,}\")\n        \n        results = quick_stability_test(\n            df,\n            feature_cols,\n            n_clusters=n_clusters,\n            n_iterations=n_iterations,\n            subsample_size=sample_size,\n            random_state=random_state\n        )\n        \n        results_by_size[sample_size] = results\n    \n    # Create summary plot\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Plot 1: Stability metrics vs sample size\n    sizes = list(results_by_size.keys())\n    mean_aris = [results_by_size[s]['mean_ari'] for s in sizes]\n    std_aris = [results_by_size[s]['std_ari'] for s in sizes]\n    mean_nmis = [results_by_size[s]['mean_nmi'] for s in sizes]\n    std_nmis = [results_by_size[s]['std_nmi'] for s in sizes]\n    \n    ax1.errorbar(sizes, mean_aris, yerr=std_aris, marker='o', label='ARI', capsize=5, markersize=8)\n    ax1.errorbar(sizes, mean_nmis, yerr=std_nmis, marker='s', label='NMI', capsize=5, markersize=8)\n    \n    ax1.set_xlabel('Sample Size')\n    ax1.set_ylabel('Mean Stability Score')\n    ax1.set_title('Stability vs Sample Size')\n    ax1.set_xscale('log')\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n    ax1.set_ylim(0, 1.05)\n    \n    # Plot 2: Centroid distance vs sample size\n    mean_dists = [results_by_size[s]['mean_centroid_dist'] for s in sizes]\n    std_dists = [results_by_size[s]['std_centroid_dist'] for s in sizes]\n    \n    ax2.errorbar(sizes, mean_dists, yerr=std_dists, marker='d', color='green', capsize=5, markersize=8)\n    ax2.set_xlabel('Sample Size')\n    ax2.set_ylabel('Mean Centroid Distance')\n    ax2.set_title('Centroid Stability vs Sample Size')\n    ax2.set_xscale('log')\n    ax2.grid(True, alpha=0.3)\n    \n    plt.suptitle('K-Modes Stability Analysis Across Sample Sizes', fontsize=16)\n    plt.tight_layout()\n    \n    return results_by_size\n\n# Run multi-scale analysis with very small samples for quick testing\nprint(\"Running multi-scale stability analysis...\")\nmulti_results = multi_scale_stability_analysis(\n    bfrss_raw_df,\n    DEMOGRAPHIC_FEATURE_COLUMNS,\n    n_clusters=2,\n    sample_sizes=[500, 1000, 2000, 5000],  # Small sizes for quick testing\n    n_iterations=5\n)",
   "metadata": {},
   "outputs": [],
   "execution_count": null,
   "id": "a6a397fd17b6778"
  },
  {
   "cell_type": "code",
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": "# Define demographic features and health condition targets\n\n# Demographic features from the visualization notebook\nDEMOGRAPHIC_FEATURES = [\n    'MARITAL',    # Marital status\n    'EDUCA',      # Education level\n    'RENTHOM1',   # Home ownership\n    'VETERAN3',   # Veteran status\n    'EMPLOY1',    # Employment status\n    'INCOME3',    # Income level\n    'SEXVAR',     # Sex/Gender\n    '_HISPANC',   # Hispanic ethnicity\n    '_IMPRACE',   # Race\n]\n\n# Chronic health condition features to predict\n# These are common chronic conditions in BRFSS data\nCHRONIC_HEALTH_CONDITIONS = [\n    'DIABETE4',   # Diabetes\n    'CVDINFR4',   # Heart attack\n    'CVDCRHD4',   # Coronary heart disease\n    'CVDSTRK3',   # Stroke\n    'ASTHMA3',    # Asthma\n    'CHCSCNC1',   # Skin cancer\n    'CHCOCNC1',   # Other cancer\n    'CHCKDNY2',   # Kidney disease\n    'CHCCOPD3',   # COPD\n    'HAVARTH5',   # Arthritis\n    'ADDEPEV3',   # Depression\n    'DECIDE',     # Cognitive decline\n]\n\n# Healthcare access features (for comparison with RQ1)\nHEALTHCARE_ACCESS_FEATURES = [\n    'HLTHPLN1',   # Health insurance\n    'PERSDOC3',   # Personal doctor\n    'MEDCOST1',   # Couldn't see doctor due to cost\n    'CHECKUP1',   # Time since last checkup\n]\n\nprint(f\"Demographic features ({len(DEMOGRAPHIC_FEATURES)}): {DEMOGRAPHIC_FEATURES}\")\nprint(f\"\\\\nChronic health conditions ({len(CHRONIC_HEALTH_CONDITIONS)}): {CHRONIC_HEALTH_CONDITIONS}\")\nprint(f\"\\\\nHealthcare access features ({len(HEALTHCARE_ACCESS_FEATURES)}): {HEALTHCARE_ACCESS_FEATURES}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bp18rpnbfou",
   "source": "# Example usage and batch prediction function\ndef run_multiple_predictions(df, feature_columns, target_columns, \n                            model_params=None, training_params=None):\n    \"\"\"\n    Run predictions for multiple target variables using the same feature set.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Input dataframe\n    feature_columns : list\n        List of feature column names\n    target_columns : list\n        List of target column names to predict\n    model_params : dict, optional\n        Model parameters\n    training_params : dict, optional\n        Training parameters\n        \n    Returns:\n    --------\n    results : dict\n        Dictionary with results for each target variable\n    \"\"\"\n    \n    all_results = {}\n    \n    for target_col in target_columns:\n        print(f\"\\\\n\\\\n{'='*80}\")\n        print(f\"RUNNING PREDICTION FOR: {target_col}\")\n        print(f\"{'='*80}\")\n        \n        try:\n            result = run_tensorflow_prediction(\n                df, feature_columns, target_col, \n                model_params=model_params, \n                training_params=training_params\n            )\n            \n            if result is not None:\n                all_results[target_col] = result\n                print(f\"\\\\n✓ Successfully completed prediction for {target_col}\")\n                print(f\"  - Accuracy: {result['evaluation']['test_accuracy']:.4f}\")\n                if 'auc' in result['evaluation']:\n                    print(f\"  - AUC: {result['evaluation']['auc']:.4f}\")\n            else:\n                print(f\"\\\\n✗ Failed to complete prediction for {target_col}\")\n                \n        except Exception as e:\n            print(f\"\\\\n✗ Error predicting {target_col}: {str(e)}\")\n            continue\n    \n    # Summary\n    print(f\"\\\\n\\\\n{'='*80}\")\n    print(f\"PREDICTION SUMMARY\")\n    print(f\"{'='*80}\")\n    print(f\"Total targets attempted: {len(target_columns)}\")\n    print(f\"Successful predictions: {len(all_results)}\")\n    print(f\"\\\\nResults summary:\")\n    \n    for target, result in all_results.items():\n        accuracy = result['evaluation']['test_accuracy']\n        auc = result['evaluation'].get('auc', 'N/A')\n        print(f\"  {target:15s} - Accuracy: {accuracy:.4f}, AUC: {auc}\")\n    \n    return all_results\n\n# Quick example of how to use the functions\nprint(\"\\\\n\" + \"=\"*60)\nprint(\"TENSORFLOW ML PREDICTION SETUP COMPLETE\")\nprint(\"=\"*60)\nprint(\"\\\\nAvailable arrays for predictions:\")\nprint(f\"\\\\nDEMOGRAPHIC_FEATURES ({len(DEMOGRAPHIC_FEATURES)} features):\")\nfor i, feature in enumerate(DEMOGRAPHIC_FEATURES):\n    print(f\"  {i+1:2d}. {feature}\")\n\nprint(f\"\\\\nCHRONIC_HEALTH_CONDITIONS ({len(CHRONIC_HEALTH_CONDITIONS)} targets):\")\nfor i, condition in enumerate(CHRONIC_HEALTH_CONDITIONS):\n    print(f\"  {i+1:2d}. {condition}\")\n\nprint(f\"\\\\nHEALTHCARE_ACCESS_FEATURES ({len(HEALTHCARE_ACCESS_FEATURES)} targets):\")\nfor i, feature in enumerate(HEALTHCARE_ACCESS_FEATURES):\n    print(f\"  {i+1:2d}. {feature}\")\n\nprint(\"\\\\n\" + \"=\"*60)\nprint(\"USAGE EXAMPLES:\")\nprint(\"=\"*60)\nprint(\"\"\"\n# Example 1: Single prediction\nresults = run_tensorflow_prediction(\n    bfrss_raw_df, \n    DEMOGRAPHIC_FEATURES, \n    'DIABETE4'  # Predict diabetes\n)\n\n# Example 2: Multiple predictions\nresults = run_multiple_predictions(\n    bfrss_raw_df,\n    DEMOGRAPHIC_FEATURES,\n    ['DIABETE4', 'CVDINFR4', 'ASTHMA3']  # Predict multiple conditions\n)\n\n# Example 3: Custom model parameters\ncustom_model_params = {\n    'hidden_layers': [128, 64, 32],\n    'dropout_rate': 0.4,\n    'learning_rate': 0.0005\n}\nresults = run_tensorflow_prediction(\n    bfrss_raw_df, \n    DEMOGRAPHIC_FEATURES, \n    'DIABETE4',\n    model_params=custom_model_params\n)\n\"\"\")\nprint(\"=\"*60)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "xiajm0ks1h",
   "source": "# Main prediction function that can be called with different feature sets\ndef run_tensorflow_prediction(df, feature_columns, target_column, \n                             model_params=None, training_params=None,\n                             test_size=0.2, random_state=42):\n    \"\"\"\n    Complete pipeline for TensorFlow prediction using demographic features.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Input dataframe (BRFSS data)\n    feature_columns : list\n        List of feature column names to use for prediction\n    target_column : str\n        Name of target column to predict\n    model_params : dict, optional\n        Model parameters (hidden_layers, dropout_rate, learning_rate)\n    training_params : dict, optional\n        Training parameters (epochs, batch_size, validation_split)\n    test_size : float\n        Proportion of data to use for testing\n    random_state : int\n        Random seed for reproducibility\n        \n    Returns:\n    --------\n    results : dict\n        Dictionary containing model, history, evaluation metrics, and data splits\n    \"\"\"\n    \n    print(f\"\\\\n{'='*60}\")\n    print(f\"TensorFlow ML Prediction: {target_column}\")\n    print(f\"Features: {feature_columns}\")\n    print(f\"{'='*60}\")\n    \n    # Default parameters\n    if model_params is None:\n        model_params = {\n            'hidden_layers': [64, 32, 16],\n            'dropout_rate': 0.3,\n            'learning_rate': 0.001\n        }\n    \n    if training_params is None:\n        training_params = {\n            'epochs': 50,\n            'batch_size': 32,\n            'validation_split': 0.2,\n            'verbose': 1\n        }\n    \n    # Check if target column exists\n    if target_column not in df.columns:\n        print(f\"Error: Target column '{target_column}' not found in dataframe\")\n        return None\n    \n    # Check if all feature columns exist\n    missing_features = [col for col in feature_columns if col not in df.columns]\n    if missing_features:\n        print(f\"Error: Feature columns not found: {missing_features}\")\n        return None\n    \n    # Preprocess data\n    print(\"\\\\nPreprocessing data...\")\n    X, y, encoders, scaler = preprocess_data_for_ml(df, feature_columns, target_column)\n    \n    print(f\"Data shape after preprocessing: X={X.shape}, y={y.shape}\")\n    print(f\"Target distribution: {np.bincount(y)}\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=random_state, stratify=y\n    )\n    \n    print(f\"Train set: {X_train.shape[0]} samples\")\n    print(f\"Test set: {X_test.shape[0]} samples\")\n    \n    # Build model\n    print(\"\\\\nBuilding neural network...\")\n    model = build_neural_network(\n        input_dim=X_train.shape[1],\n        output_dim=1,  # Binary classification\n        **model_params\n    )\n    \n    print(f\"Model architecture:\")\n    model.summary()\n    \n    # Train model\n    print(\"\\\\nTraining model...\")\n    history, evaluation = train_and_evaluate_model(\n        model, X_train, y_train, X_test, y_test, **training_params\n    )\n    \n    # Print evaluation results\n    print_evaluation_report(\n        y_test, evaluation['y_pred'], evaluation['y_pred_prob'].flatten(), \n        target_name=target_column\n    )\n    \n    # Create visualizations\n    print(\"\\\\nGenerating visualizations...\")\n    \n    # Training history\n    plot_training_history(history, f\"{target_column} - \")\n    plt.show()\n    \n    # Confusion matrix and ROC curve\n    plot_confusion_matrix_and_roc(\n        y_test, evaluation['y_pred'], evaluation['y_pred_prob'].flatten(),\n        f\"{target_column} - \"\n    )\n    plt.show()\n    \n    # Return results\n    results = {\n        'model': model,\n        'history': history,\n        'evaluation': evaluation,\n        'encoders': encoders,\n        'scaler': scaler,\n        'X_train': X_train,\n        'X_test': X_test,\n        'y_train': y_train,\n        'y_test': y_test,\n        'feature_columns': feature_columns,\n        'target_column': target_column\n    }\n    \n    return results",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "g3mijuvucir",
   "source": "# Visualization functions for model evaluation\ndef plot_training_history(history, title_prefix=\"\"):\n    \"\"\"\n    Plot training history including loss and accuracy curves.\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    \n    # Plot training & validation loss\n    axes[0].plot(history.history['loss'], label='Training Loss')\n    axes[0].plot(history.history['val_loss'], label='Validation Loss')\n    axes[0].set_title(f'{title_prefix}Model Loss')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # Plot training & validation accuracy\n    axes[1].plot(history.history['accuracy'], label='Training Accuracy')\n    axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy')\n    axes[1].set_title(f'{title_prefix}Model Accuracy')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Accuracy')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    return fig\n\ndef plot_confusion_matrix_and_roc(y_true, y_pred, y_pred_prob, title_prefix=\"\"):\n    \"\"\"\n    Plot confusion matrix and ROC curve for binary classification.\n    \"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_true, y_pred)\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n    axes[0].set_title(f'{title_prefix}Confusion Matrix')\n    axes[0].set_xlabel('Predicted')\n    axes[0].set_ylabel('Actual')\n    \n    # ROC Curve\n    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n    auc = roc_auc_score(y_true, y_pred_prob)\n    \n    axes[1].plot(fpr, tpr, linewidth=2, label=f'ROC Curve (AUC = {auc:.3f})')\n    axes[1].plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)\n    axes[1].set_xlabel('False Positive Rate')\n    axes[1].set_ylabel('True Positive Rate')\n    axes[1].set_title(f'{title_prefix}ROC Curve')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    return fig\n\ndef print_evaluation_report(y_true, y_pred, y_pred_prob, target_name=\"Target\"):\n    \"\"\"\n    Print comprehensive evaluation report.\n    \"\"\"\n    print(f\"\\\\n=== {target_name} Prediction Results ===\")\n    print(f\"Accuracy: {(y_pred == y_true).mean():.4f}\")\n    \n    if len(np.unique(y_true)) == 2:\n        auc = roc_auc_score(y_true, y_pred_prob)\n        print(f\"AUC-ROC: {auc:.4f}\")\n    \n    print(f\"\\\\nClassification Report:\")\n    print(classification_report(y_true, y_pred))\n    \n    # Class distribution\n    unique, counts = np.unique(y_true, return_counts=True)\n    print(f\"\\\\nClass Distribution (Test Set):\")\n    for class_val, count in zip(unique, counts):\n        print(f\"  Class {class_val}: {count} ({count/len(y_true)*100:.1f}%)\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5jo3b637y5g",
   "source": "# TensorFlow Neural Network Model Builder\ndef build_neural_network(input_dim, output_dim=1, hidden_layers=[64, 32, 16], \n                        dropout_rate=0.3, learning_rate=0.001):\n    \"\"\"\n    Build a neural network for classification using TensorFlow/Keras.\n    \n    Parameters:\n    -----------\n    input_dim : int\n        Number of input features\n    output_dim : int\n        Number of output classes (1 for binary classification)\n    hidden_layers : list\n        List of hidden layer sizes\n    dropout_rate : float\n        Dropout rate for regularization\n    learning_rate : float\n        Learning rate for optimizer\n    \n    Returns:\n    --------\n    model : keras.Sequential\n        Compiled neural network model\n    \"\"\"\n    \n    model = keras.Sequential()\n    \n    # Input layer\n    model.add(keras.layers.Dense(hidden_layers[0], activation='relu', \n                                input_shape=(input_dim,)))\n    model.add(keras.layers.BatchNormalization())\n    model.add(keras.layers.Dropout(dropout_rate))\n    \n    # Hidden layers\n    for units in hidden_layers[1:]:\n        model.add(keras.layers.Dense(units, activation='relu'))\n        model.add(keras.layers.BatchNormalization())\n        model.add(keras.layers.Dropout(dropout_rate))\n    \n    # Output layer\n    if output_dim == 1:\n        # Binary classification\n        model.add(keras.layers.Dense(1, activation='sigmoid'))\n        loss = 'binary_crossentropy'\n        metrics = ['accuracy', keras.metrics.AUC(name='auc')]\n    else:\n        # Multi-class classification\n        model.add(keras.layers.Dense(output_dim, activation='softmax'))\n        loss = 'sparse_categorical_crossentropy'\n        metrics = ['accuracy']\n    \n    # Compile model\n    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    \n    return model\n\n# Function to train and evaluate model\ndef train_and_evaluate_model(model, X_train, y_train, X_test, y_test, \n                           epochs=50, batch_size=32, validation_split=0.2,\n                           verbose=1):\n    \"\"\"\n    Train and evaluate a neural network model.\n    \n    Returns:\n    --------\n    history : keras.callbacks.History\n        Training history\n    evaluation : dict\n        Test set evaluation metrics\n    \"\"\"\n    \n    # Early stopping callback\n    early_stopping = keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=10,\n        restore_best_weights=True\n    )\n    \n    # Reduce learning rate on plateau\n    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.5,\n        patience=5,\n        min_lr=1e-6\n    )\n    \n    # Train model\n    history = model.fit(\n        X_train, y_train,\n        epochs=epochs,\n        batch_size=batch_size,\n        validation_split=validation_split,\n        callbacks=[early_stopping, reduce_lr],\n        verbose=verbose\n    )\n    \n    # Evaluate on test set\n    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)[:2]\n    \n    # Get predictions\n    y_pred_prob = model.predict(X_test)\n    y_pred = (y_pred_prob > 0.5).astype(int).flatten()\n    \n    evaluation = {\n        'test_loss': test_loss,\n        'test_accuracy': test_accuracy,\n        'y_pred': y_pred,\n        'y_pred_prob': y_pred_prob\n    }\n    \n    # Calculate AUC if binary classification\n    if len(np.unique(y_test)) == 2:\n        evaluation['auc'] = roc_auc_score(y_test, y_pred_prob)\n    \n    return history, evaluation",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
