{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('./data/LLCP2023.parquet')\n",
    "df_mapped = pd.read_parquet('./data/LLCP2023_partialmap.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.head(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Friendly Mapping\n",
    "This creates a lookup dictionary between a column and the metadata the codebook provides on it.\n",
    "\n",
    "With this we can lookup what a value at a row/column (such as 1) 'means'.\n",
    "\n",
    "My intent was to add additional metadata there to support further EDA and analysis.\n",
    "```python\n",
    "# Example\n",
    "```\n",
    "- This create a lookup object between columns and 'friendly names'\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the metadata parser\n",
    "from pathlib import Path\n",
    "from metadata.parser import parse_codebook_html\n",
    "\n",
    "# Parse the codebook HTML file\n",
    "codebook_path = Path('./data/codebook_USCODE23_LLCP_021924.HTML')\n",
    "column_metadata = parse_codebook_html(codebook_path)\n",
    "\n",
    "# Display the number of columns parsed\n",
    "print(f\"Parsed {len(column_metadata)} column definitions from the codebook\")\n",
    "\n",
    "# Show a sample of the metadata\n",
    "sample_keys = list(column_metadata.keys())[:5]\n",
    "for key in sample_keys:\n",
    "    metadata = column_metadata[key]\n",
    "    print(f\"\\n{key}:\")\n",
    "    print(f\"  Label: {metadata.label}\")\n",
    "    print(f\"  Question: {metadata.question}\")\n",
    "    print(f\"  Column: {metadata.column}\")\n",
    "    print(f\"  Type: {metadata.type_of_variable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Examining Metadata\n",
    "At this point we should have the metadata about columns extracted.\n",
    "\n",
    "Right now it is a dictionary where column name is key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how complete the metadata is\n",
    "print(f\"Total columns in dataframe: {len(df.columns)}\")\n",
    "print(f\"Total metadata parsed: {len(column_metadata)}\")\n",
    "print(f\"Coverage: {len(column_metadata) / len(df.columns) * 100:.1f}%\")\n",
    "\n",
    "# Check which columns don't have metadata\n",
    "missing_metadata = [col for col in df.columns if col not in column_metadata]\n",
    "print(f\"\\nColumns without metadata: {len(missing_metadata)}\")\n",
    "if missing_metadata:\n",
    "    print(\"First 10 missing:\", missing_metadata[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "To be a bit more data science oriented we'll turn the dictionary into another dataframe:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Understanding the Friendly Mapping Feature\n",
    "\n",
    "The metadata parser includes a powerful \"friendly mapping\" feature that translates numeric codes in the dataset to their human-readable descriptions. This is particularly useful for categorical variables where numeric codes represent specific responses.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "Each `ColumnMetadata` object contains a `value_lookup` dictionary that maps numeric values (or None) to their text descriptions. This mapping is automatically extracted from the codebook HTML file during parsing.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **`value_lookup` dictionary**: Found in each `ColumnMetadata` object\n",
    "   - Keys: Numeric codes (int) or None\n",
    "   - Values: Human-readable descriptions (str)\n",
    "\n",
    "2. **Automatic extraction**: The `get_value_lookup()` function in `parser.py` extracts these mappings from HTML tables in the codebook\n",
    "\n",
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Understanding what values mean for a specific column\n",
    "# Let's look at the _STATE column which has distinct state codes\n",
    "\n",
    "state_metadata = column_metadata['_STATE']\n",
    "print(f\"Column: {state_metadata.sas_variable_name}\")\n",
    "print(f\"Label: {state_metadata.label}\")\n",
    "print(f\"Question: {state_metadata.question}\")\n",
    "print(f\"\\nSample of value mappings (first 10):\")\n",
    "# Show first 10 state mappings\n",
    "for i, (value, description) in enumerate(state_metadata.value_lookup.items()):\n",
    "    if i < 10:\n",
    "        print(f\"  {value}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Translating values in your data\n",
    "# Let's translate some actual STATE values from the dataframe\n",
    "\n",
    "# Get a sample of state values\n",
    "sample_values = df['_STATE'].value_counts().head(10)\n",
    "print(\"Top 10 states by number of respondents:\\n\")\n",
    "\n",
    "for value, count in sample_values.items():\n",
    "    # Get the description from value_lookup\n",
    "    description = state_metadata.value_lookup.get(int(value) if not pd.isna(value) else None, \"Unknown\")\n",
    "    print(f\"Code {int(value)}: {description} (Count: {count:,})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Creating a mapping function for easy translation\n",
    "def translate_column_values(df, column_name, metadata_dict):\n",
    "    \"\"\"\n",
    "    Translate numeric codes to descriptions for a specific column.\n",
    "    \n",
    "    Args:\n",
    "        df: The dataframe containing the data\n",
    "        column_name: Name of the column to translate\n",
    "        metadata_dict: Dictionary of column metadata\n",
    "    \n",
    "    Returns:\n",
    "        Pandas Series with translated values\n",
    "    \"\"\"\n",
    "    if column_name not in metadata_dict:\n",
    "        print(f\"No metadata found for column: {column_name}\")\n",
    "        return df[column_name]\n",
    "    \n",
    "    metadata = metadata_dict[column_name]\n",
    "    \n",
    "    # Create translation function\n",
    "    def translate(value):\n",
    "        if pd.isna(value):\n",
    "            return \"Missing\"\n",
    "        return metadata.value_lookup.get(int(value), f\"Unknown code: {value}\")\n",
    "    \n",
    "    return df[column_name].apply(translate)\n",
    "\n",
    "# Example usage - translate STATE codes\n",
    "df['STATE_NAME'] = translate_column_values(df, '_STATE', column_metadata)\n",
    "\n",
    "# Show sample\n",
    "print(\"Sample of translated state values:\")\n",
    "print(df[['_STATE', 'STATE_NAME']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 4: Working with columns that have ranges\n",
    "# Now let's test with POORHLTH which has a range value \"1 - 30\"\n",
    "\n",
    "# Re-parse the metadata with the updated parser\n",
    "from metadata.parser import parse_codebook_html\n",
    "column_metadata = parse_codebook_html(codebook_path)\n",
    "\n",
    "poorhlth_metadata = column_metadata['POORHLTH']\n",
    "print(f\"Column: {poorhlth_metadata.sas_variable_name}\")\n",
    "print(f\"Label: {poorhlth_metadata.label}\")\n",
    "\n",
    "# Check if the range was properly expanded\n",
    "print(f\"\\nTotal value mappings: {len(poorhlth_metadata.value_lookup)}\")\n",
    "print(\"\\nSample mappings:\")\n",
    "# Show some specific values to verify range expansion\n",
    "for value in [1, 15, 30, 77, 88, 99]:\n",
    "    if value in poorhlth_metadata.value_lookup:\n",
    "        print(f\"  {value}: {poorhlth_metadata.value_lookup[value]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Batch translation of multiple columns\n",
    "# This example shows how to efficiently translate multiple columns at once\n",
    "\n",
    "def batch_translate_columns(df, column_list, metadata_dict):\n",
    "    \"\"\"\n",
    "    Translate multiple columns from numeric codes to descriptions.\n",
    "    \n",
    "    Args:\n",
    "        df: The dataframe containing the data\n",
    "        column_list: List of column names to translate\n",
    "        metadata_dict: Dictionary of column metadata\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of translated series\n",
    "    \"\"\"\n",
    "    translated = {}\n",
    "    \n",
    "    for col in column_list:\n",
    "        if col in metadata_dict and col in df.columns:\n",
    "            translated[f\"{col}_DESC\"] = translate_column_values(df, col, metadata_dict)\n",
    "            print(f\"Translated {col}\")\n",
    "        else:\n",
    "            print(f\"Skipped {col} (not found in metadata or dataframe)\")\n",
    "    \n",
    "    return translated\n",
    "\n",
    "# Translate several categorical columns\n",
    "columns_to_translate = ['_STATE', 'FMONTH', 'DISPCODE', 'SEX1']\n",
    "translations = batch_translate_columns(df, columns_to_translate, column_metadata)\n",
    "\n",
    "# Add translations to dataframe\n",
    "for col_name, translated_series in translations.items():\n",
    "    df[col_name] = translated_series\n",
    "\n",
    "# Show sample of multiple translations\n",
    "print(\"\\nSample of translated data:\")\n",
    "original_cols = columns_to_translate[:3]  # Show first 3\n",
    "desc_cols = [f\"{col}_DESC\" for col in original_cols]\n",
    "print(df[original_cols + desc_cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Advanced Usage Tips (NOTE these tips were ChatGPT generated)\n",
    "\n",
    "### 1. Filtering Data by Descriptions\n",
    "Once you have the friendly mappings, you can filter data using human-readable criteria:\n",
    "\n",
    "```python\n",
    "# Find all respondents from California\n",
    "california_code = next(k for k, v in state_metadata.value_lookup.items() if 'California' in v)\n",
    "ca_data = df[df['_STATE'] == california_code]\n",
    "```\n",
    "\n",
    "### 2. Creating Analysis-Ready DataFrames\n",
    "The friendly mappings are especially useful when creating subsets for analysis:\n",
    "\n",
    "```python\n",
    "# Create a subset with translated categorical variables\n",
    "analysis_df = df[['_STATE', 'SEX1', 'POORHLTH']].copy()\n",
    "for col in analysis_df.columns:\n",
    "    if col in column_metadata:\n",
    "        analysis_df[f'{col}_desc'] = translate_column_values(analysis_df, col, column_metadata)\n",
    "```\n",
    "\n",
    "### 3. Handling Special Values\n",
    "Many BRFSS variables use special codes like:\n",
    "- 77: Don't know/Not sure\n",
    "- 88: None (for days-based questions)\n",
    "- 99: Refused\n",
    "\n",
    "The value_lookup dictionary includes these, making it easy to identify and handle them appropriately in your analysis.\n",
    "\n",
    "### 4. Integration with Existing Mapped Data\n",
    "The `LLCP2023_partialmap.parquet` file already has some columns translated. You can use the metadata parser to:\n",
    "- Verify existing mappings\n",
    "- Add mappings for additional columns\n",
    "- Create custom mapping schemes for specific analyses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
